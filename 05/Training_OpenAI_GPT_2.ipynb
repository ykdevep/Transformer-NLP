{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LH2YgC7LfzJZ"
      },
      "source": [
        "#Training OpenAI GTP-2\n",
        "Copyright 2020, MIT License. Denis Rothman created the Colab notebook using the OpenAI repository, adding title steps for educational purposes only.\n",
        "\n",
        "IMPORTANT NOTE: GPT-2 has conversational chatbot functionality. This notebook was designed to illustrate text generation with GPT-2. However, Google Colab does not support Tensforflow 1x anymore.\n",
        "\n",
        "\n",
        "There are a solution:\n",
        "\n",
        "1. Solution 1:  Running this notebook locally after installing Tensforflow 1. x on a local machine and setting up the directories as defined in the notebook.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "It is important to note that we are running a low-level GPT-2 model and not a one-line call to obtain a result. We are also avoiding pre-packaged versions. We are getting our hands dirty to understand the architecture of a GPT-2 from scratch. You might get some deprecation messages. However, the effort is worthwhile.\n",
        "\n",
        "***Code References***\n",
        "\n",
        "[Reference: OpenAI Repository](https://github.com/openai/gpt-2)\n",
        "The repository was cloned and adapted to N Shepperd's repository.\n",
        "\n",
        "[Reference: N Shepperd Repository](https://github.com/nshepperd/gpt-2)\n",
        "The repository was not cloned. N Shepperd's training programs were inserted into the OpenAI Repository. The list of N Shepperd's programs are cited in the 'N Shepperd' section of the notebook. Some programs were modified for educational purposes only to work with this notebook.\n",
        "\n",
        "***Model Reference Paper***\n",
        "\n",
        "[Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,2019,'Language Models are Unsupervised Multitask Learners'](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n",
        "\n",
        "\n",
        "***Step 1: Pre-requisites:***\n",
        "\n",
        "a) activate GPU in the notebook settings runTime menu <br>\n",
        "b) Upload the following program files and dset.txt(dataset) with the file manager: train.py,load_dataset.py,encode.py,accumulate.py,memory_saving_gradients.py,dset.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZskjWH2AFA2I"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image     #This is used for rendering images in the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Pv7XdZElRx"
      },
      "source": [
        "#Steps 2 to 6: Initial steps of the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isqdu1fpfmqM",
        "outputId": "350862e0-eda7-4f40-952b-628b7e8e04e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 7.84 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ]
        }
      ],
      "source": [
        "#@title Step 2: Cloning the OpenAI GPT-2 Repository \n",
        "#!git clone https://github.com/nshepperd/gpt-2.git\n",
        "!git clone https://github.com/openai/gpt-2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RHOjN-TjUbj",
        "outputId": "db0f0020-802e-423d-9d62-5fbf82de96fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.0MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.4MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2021.5.30)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=e7fc9c87b2d5efd0768028e123e2fb967ad68cbefae7cde100e5d94495ce99ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534399 sha256=f70dd55b248580c4d3285a1c31240958a9cf3be653fc138c4087fd2011464117\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ]
        }
      ],
      "source": [
        "#@title Step 3: Installing the requirements\n",
        "import os                     # when the VM restarts import os necessary\n",
        "os.chdir(\"/content/gpt-2\")    \n",
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9vV73Opw68m",
        "outputId": "93eb4c21-cd7c-4474-af35-468292941e63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n",
            "Installing collected packages: toposort\n",
            "Successfully installed toposort-1.6\n"
          ]
        }
      ],
      "source": [
        "!pip install toposort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kpNCnh9fyYD",
        "outputId": "85d513e4-650c-4a46-9a25-9f4ed62610e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "#@title Step 4: Checking TensorFlow version \n",
        "#Colab has tf 1.x and tf 2.x installed\n",
        "#Restart runtime using 'Runtime' -> 'Restart runtime...'\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvVj0cLVkaPL",
        "outputId": "9b4faf72-3adf-42b0-bb59-2e7dbc0e1f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 1.19Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.95Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.03Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 28.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 6.01Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.78Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.81Mit/s]                                                       \n"
          ]
        }
      ],
      "source": [
        "#@title Step 5: Downloading 117M parameter GPT-2 Model\n",
        "# run code and send argument\n",
        "import os # after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2\")\n",
        "!python3 download_model.py '117M' #creates model directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV5K8rvD1b-r"
      },
      "outputs": [],
      "source": [
        "#@title Step 6: Copying the Project Resources to scr\n",
        "!cp /content/dset.txt /content/gpt-2/src/\n",
        "!cp -r /content/gpt-2/models/ /content/gpt-2/src/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0rKgvYTEq8c"
      },
      "source": [
        "#Step 7: The N Shepperd training files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTUxDwtWlOLf"
      },
      "outputs": [],
      "source": [
        "#@title Step 7: Copying the N Shepperd Training Files\n",
        "#Referfence GitHub repository: https://github.com/nshepperd/gpt-2\n",
        "import os # import after runtime is restarted\n",
        "!cp /content/train.py /content/gpt-2/src/\n",
        "!cp /content/load_dataset.py /content/gpt-2/src/\n",
        "!cp /content/encode.py /content/gpt-2/src/\n",
        "!cp /content/accumulate.py /content/gpt-2/src/\n",
        "!cp /content/memory_saving_gradients.py /content/gpt-2/src/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pumMm6csEukO"
      },
      "source": [
        "#Step 8: Encoding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6T2OrWoOvG0",
        "outputId": "0838e7d0-2801-4783-c08d-f961962947f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading files\n",
            "100% 1/1 [00:10<00:00, 10.06s/it]\n",
            "Writing out.npz\n"
          ]
        }
      ],
      "source": [
        "#@title Step 8:Encoding dataset\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "model_name=\"117M\"\n",
        "!python /content/gpt-2/src/encode.py dset.txt out.npz "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzlkNGbAkDBk"
      },
      "source": [
        "#Step 9: Training a GPT-2 model\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "!python train.py --dataset out.npz\n",
        "\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "!python train.py --dataset out.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIc_acgKYQvs",
        "outputId": "751c6e42-c5a7-4dce-b01e-7712f60a1b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:89: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:92: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-06-17 09:50:58.181502: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2021-06-17 09:50:58.191275: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000194999 Hz\n",
            "2021-06-17 09:50:58.191578: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613f34ef480 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-17 09:50:58.191613: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-06-17 09:50:58.195851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-06-17 09:50:58.372484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:58.373188: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5613f34eed80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-17 09:50:58.373217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-06-17 09:50:58.374296: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:58.374924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-06-17 09:50:58.389231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-06-17 09:50:58.621743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-06-17 09:50:58.742569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-06-17 09:50:58.808786: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-06-17 09:50:59.032628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-06-17 09:50:59.078705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-06-17 09:50:59.579306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-06-17 09:50:59.579495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:59.580137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:59.580663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-06-17 09:50:59.583137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-06-17 09:50:59.584520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-06-17 09:50:59.584551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-06-17 09:50:59.584562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-06-17 09:50:59.585667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:59.586337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 09:50:59.586907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From train.py:93: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From train.py:118: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:122: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:145: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:148: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:150: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:153: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From train.py:157: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00, 12.40it/s]\n",
            "dataset has 2548152 tokens\n",
            "Training...\n",
            "2021-06-17 09:51:23.231614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "[1 | 4.69] loss=2.98 avg=2.98\n",
            "[2 | 5.11] loss=3.36 avg=3.17\n",
            "[3 | 5.53] loss=3.66 avg=3.34\n",
            "[4 | 5.96] loss=1.44 avg=2.85\n",
            "[5 | 6.38] loss=3.59 avg=3.00\n",
            "[6 | 6.80] loss=3.24 avg=3.05\n",
            "[7 | 7.23] loss=3.47 avg=3.11\n",
            "[8 | 7.66] loss=3.39 avg=3.14\n",
            "[9 | 8.08] loss=3.12 avg=3.14\n",
            "[10 | 8.51] loss=3.32 avg=3.16\n",
            "[11 | 8.93] loss=3.03 avg=3.15\n",
            "[12 | 9.35] loss=3.27 avg=3.16\n",
            "[13 | 9.78] loss=3.22 avg=3.16\n",
            "[14 | 10.20] loss=3.02 avg=3.15\n",
            "[15 | 10.63] loss=3.23 avg=3.16\n",
            "[16 | 11.05] loss=3.24 avg=3.16\n",
            "[17 | 11.48] loss=3.30 avg=3.17\n",
            "[18 | 11.90] loss=3.27 avg=3.18\n",
            "[19 | 12.33] loss=3.39 avg=3.19\n",
            "[20 | 12.75] loss=3.42 avg=3.20\n",
            "[21 | 13.18] loss=3.05 avg=3.20\n",
            "[22 | 13.61] loss=3.17 avg=3.19\n",
            "[23 | 14.03] loss=3.22 avg=3.20\n",
            "[24 | 14.46] loss=3.25 avg=3.20\n",
            "[25 | 14.88] loss=3.30 avg=3.20\n",
            "[26 | 15.31] loss=3.14 avg=3.20\n",
            "[27 | 15.74] loss=3.20 avg=3.20\n",
            "[28 | 16.17] loss=3.24 avg=3.20\n",
            "[29 | 16.59] loss=3.00 avg=3.19\n",
            "[30 | 17.02] loss=3.09 avg=3.19\n",
            "[31 | 17.45] loss=1.63 avg=3.13\n",
            "[32 | 17.87] loss=3.24 avg=3.13\n",
            "[33 | 18.29] loss=3.08 avg=3.13\n",
            "[34 | 18.72] loss=3.02 avg=3.13\n",
            "[35 | 19.15] loss=3.26 avg=3.13\n",
            "[36 | 19.58] loss=3.13 avg=3.13\n",
            "[37 | 20.01] loss=2.92 avg=3.13\n",
            "[38 | 20.44] loss=2.98 avg=3.12\n",
            "[39 | 20.86] loss=3.17 avg=3.12\n",
            "[40 | 21.29] loss=3.30 avg=3.13\n",
            "[41 | 21.72] loss=3.26 avg=3.13\n",
            "[42 | 22.15] loss=3.09 avg=3.13\n",
            "[43 | 22.57] loss=3.31 avg=3.14\n",
            "[44 | 23.00] loss=3.31 avg=3.14\n",
            "[45 | 23.43] loss=3.08 avg=3.14\n",
            "[46 | 23.85] loss=2.89 avg=3.13\n",
            "[47 | 24.28] loss=3.38 avg=3.14\n",
            "[48 | 24.71] loss=3.15 avg=3.14\n",
            "[49 | 25.14] loss=3.28 avg=3.14\n",
            "[50 | 25.57] loss=3.18 avg=3.14\n",
            "[51 | 25.99] loss=3.11 avg=3.14\n",
            "[52 | 26.42] loss=3.18 avg=3.14\n",
            "[53 | 26.85] loss=2.96 avg=3.14\n",
            "[54 | 27.28] loss=3.38 avg=3.15\n",
            "[55 | 27.71] loss=2.99 avg=3.14\n",
            "[56 | 28.14] loss=3.19 avg=3.14\n",
            "[57 | 28.57] loss=2.96 avg=3.14\n",
            "[58 | 29.00] loss=3.28 avg=3.14\n",
            "[59 | 29.43] loss=3.30 avg=3.15\n",
            "[60 | 29.86] loss=3.34 avg=3.15\n",
            "[61 | 30.29] loss=3.24 avg=3.15\n",
            "[62 | 30.71] loss=3.28 avg=3.15\n",
            "[63 | 31.14] loss=3.39 avg=3.16\n",
            "[64 | 31.57] loss=3.22 avg=3.16\n",
            "[65 | 32.00] loss=3.15 avg=3.16\n",
            "[66 | 32.44] loss=3.04 avg=3.16\n",
            "[67 | 32.87] loss=3.18 avg=3.16\n",
            "[68 | 33.30] loss=3.37 avg=3.16\n",
            "[69 | 33.72] loss=3.05 avg=3.16\n",
            "[70 | 34.14] loss=2.99 avg=3.16\n",
            "[71 | 34.58] loss=2.55 avg=3.15\n",
            "[72 | 35.01] loss=2.98 avg=3.14\n",
            "[73 | 35.44] loss=3.16 avg=3.14\n",
            "[74 | 35.87] loss=3.01 avg=3.14\n",
            "[75 | 36.30] loss=3.18 avg=3.14\n",
            "[76 | 36.73] loss=3.21 avg=3.14\n",
            "[77 | 37.16] loss=2.81 avg=3.14\n",
            "[78 | 37.60] loss=3.64 avg=3.15\n",
            "[79 | 38.03] loss=2.90 avg=3.14\n",
            "[80 | 38.46] loss=3.06 avg=3.14\n",
            "[81 | 38.90] loss=3.07 avg=3.14\n",
            "[82 | 39.33] loss=3.01 avg=3.14\n",
            "[83 | 39.76] loss=3.12 avg=3.14\n",
            "[84 | 40.20] loss=0.83 avg=3.10\n",
            "[85 | 40.64] loss=2.79 avg=3.09\n",
            "[86 | 41.07] loss=2.49 avg=3.08\n",
            "[87 | 41.49] loss=2.81 avg=3.07\n",
            "[88 | 41.93] loss=3.20 avg=3.08\n",
            "[89 | 42.36] loss=2.91 avg=3.07\n",
            "[90 | 42.79] loss=3.03 avg=3.07\n",
            "[91 | 43.23] loss=3.24 avg=3.08\n",
            "[92 | 43.66] loss=3.07 avg=3.08\n",
            "[93 | 44.09] loss=2.81 avg=3.07\n",
            "[94 | 44.52] loss=0.73 avg=3.03\n",
            "[95 | 44.96] loss=0.74 avg=3.00\n",
            "[96 | 45.39] loss=3.11 avg=3.00\n",
            "[97 | 45.82] loss=2.19 avg=2.98\n",
            "[98 | 46.26] loss=3.49 avg=2.99\n",
            "[99 | 46.69] loss=3.20 avg=3.00\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the thesaurus.\n",
            "\n",
            "The word \"thesaurus\" is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a contraction of the word \"thesaurus,\" which is a\n",
            "\n",
            "[100 | 58.68] loss=2.87 avg=2.99\n",
            "[101 | 59.12] loss=3.34 avg=3.00\n",
            "[102 | 59.55] loss=2.71 avg=3.00\n",
            "[103 | 59.98] loss=3.28 avg=3.00\n",
            "[104 | 60.41] loss=2.93 avg=3.00\n",
            "[105 | 60.85] loss=3.25 avg=3.00\n",
            "[106 | 61.28] loss=2.99 avg=3.00\n",
            "[107 | 61.72] loss=3.28 avg=3.01\n",
            "[108 | 62.15] loss=2.56 avg=3.00\n",
            "[109 | 62.59] loss=2.93 avg=3.00\n",
            "[110 | 63.02] loss=3.04 avg=3.00\n",
            "[111 | 63.46] loss=3.04 avg=3.00\n",
            "[112 | 63.89] loss=3.25 avg=3.00\n",
            "[113 | 64.33] loss=3.37 avg=3.01\n",
            "[114 | 64.76] loss=2.84 avg=3.01\n",
            "[115 | 65.20] loss=3.35 avg=3.01\n",
            "[116 | 65.63] loss=2.98 avg=3.01\n",
            "[117 | 66.07] loss=2.97 avg=3.01\n",
            "[118 | 66.50] loss=3.17 avg=3.01\n",
            "[119 | 66.94] loss=3.04 avg=3.01\n",
            "[120 | 67.37] loss=3.02 avg=3.01\n",
            "[121 | 67.81] loss=3.03 avg=3.01\n",
            "[122 | 68.24] loss=2.86 avg=3.01\n",
            "[123 | 68.68] loss=3.02 avg=3.01\n",
            "[124 | 69.11] loss=3.25 avg=3.01\n",
            "[125 | 69.55] loss=2.98 avg=3.01\n",
            "[126 | 69.98] loss=3.14 avg=3.02\n",
            "[127 | 70.42] loss=2.87 avg=3.01\n",
            "[128 | 70.85] loss=3.06 avg=3.01\n",
            "[129 | 71.29] loss=3.11 avg=3.02\n",
            "[130 | 71.72] loss=3.21 avg=3.02\n",
            "[131 | 72.16] loss=3.13 avg=3.02\n",
            "[132 | 72.59] loss=3.00 avg=3.02\n",
            "[133 | 73.03] loss=2.81 avg=3.02\n",
            "[134 | 73.46] loss=3.00 avg=3.02\n",
            "[135 | 73.90] loss=2.35 avg=3.01\n",
            "[136 | 74.34] loss=3.08 avg=3.01\n",
            "[137 | 74.77] loss=2.78 avg=3.01\n",
            "[138 | 75.21] loss=2.97 avg=3.01\n",
            "[139 | 75.65] loss=3.22 avg=3.01\n",
            "[140 | 76.08] loss=3.06 avg=3.01\n",
            "[141 | 76.52] loss=2.86 avg=3.01\n",
            "[142 | 76.95] loss=2.97 avg=3.01\n",
            "[143 | 77.39] loss=2.86 avg=3.00\n",
            "[144 | 77.83] loss=3.10 avg=3.01\n",
            "[145 | 78.26] loss=3.18 avg=3.01\n",
            "[146 | 78.70] loss=3.03 avg=3.01\n",
            "[147 | 79.14] loss=3.02 avg=3.01\n",
            "[148 | 79.58] loss=3.19 avg=3.01\n",
            "[149 | 80.01] loss=3.12 avg=3.01\n",
            "[150 | 80.45] loss=3.02 avg=3.01\n",
            "[151 | 80.89] loss=2.98 avg=3.01\n",
            "[152 | 81.33] loss=2.99 avg=3.01\n",
            "[153 | 81.76] loss=2.99 avg=3.01\n",
            "[154 | 82.20] loss=3.15 avg=3.01\n",
            "[155 | 82.64] loss=3.38 avg=3.02\n",
            "[156 | 83.08] loss=2.96 avg=3.02\n",
            "[157 | 83.52] loss=2.95 avg=3.02\n",
            "[158 | 83.96] loss=2.93 avg=3.02\n",
            "[159 | 84.40] loss=3.14 avg=3.02\n",
            "[160 | 84.83] loss=3.02 avg=3.02\n",
            "[161 | 85.27] loss=3.03 avg=3.02\n",
            "[162 | 85.71] loss=2.90 avg=3.02\n",
            "[163 | 86.15] loss=2.91 avg=3.01\n",
            "[164 | 86.59] loss=2.35 avg=3.01\n",
            "[165 | 87.03] loss=3.25 avg=3.01\n",
            "[166 | 87.47] loss=3.53 avg=3.02\n",
            "[167 | 87.91] loss=3.30 avg=3.02\n",
            "[168 | 88.35] loss=3.15 avg=3.02\n",
            "[169 | 88.79] loss=2.93 avg=3.02\n",
            "[170 | 89.23] loss=2.91 avg=3.02\n",
            "[171 | 89.67] loss=3.12 avg=3.02\n",
            "[172 | 90.11] loss=2.77 avg=3.02\n",
            "[173 | 90.55] loss=2.97 avg=3.02\n",
            "[174 | 90.98] loss=3.25 avg=3.02\n",
            "[175 | 91.42] loss=3.10 avg=3.02\n",
            "[176 | 91.86] loss=3.11 avg=3.02\n",
            "[177 | 92.30] loss=2.97 avg=3.02\n",
            "[178 | 92.73] loss=2.85 avg=3.02\n",
            "[179 | 93.17] loss=3.10 avg=3.02\n",
            "[180 | 93.61] loss=3.05 avg=3.02\n",
            "[181 | 94.05] loss=3.17 avg=3.02\n",
            "[182 | 94.49] loss=3.02 avg=3.02\n",
            "[183 | 94.93] loss=2.86 avg=3.02\n",
            "[184 | 95.37] loss=2.81 avg=3.02\n",
            "[185 | 95.81] loss=3.12 avg=3.02\n",
            "[186 | 96.25] loss=2.75 avg=3.01\n",
            "[187 | 96.69] loss=2.98 avg=3.01\n",
            "[188 | 97.13] loss=2.91 avg=3.01\n",
            "[189 | 97.58] loss=2.96 avg=3.01\n",
            "[190 | 98.01] loss=2.85 avg=3.01\n",
            "[191 | 98.46] loss=3.04 avg=3.01\n",
            "[192 | 98.90] loss=3.07 avg=3.01\n",
            "[193 | 99.34] loss=2.94 avg=3.01\n",
            "[194 | 99.78] loss=3.09 avg=3.01\n",
            "[195 | 100.22] loss=2.88 avg=3.01\n",
            "[196 | 100.66] loss=2.95 avg=3.01\n",
            "[197 | 101.10] loss=2.90 avg=3.01\n",
            "[198 | 101.54] loss=2.99 avg=3.01\n",
            "[199 | 101.98] loss=2.93 avg=3.01\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "The first thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The second thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The third thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The fourth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The fifth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The sixth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The seventh thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The eighth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The ninth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The tenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The eleventh thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The twelfth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do something that is not intended for them.\n",
            "\n",
            "The thirteenth thing to do is to make sure that the user is not using the app to do\n",
            "\n",
            "[200 | 112.60] loss=2.77 avg=3.00\n",
            "[201 | 113.04] loss=2.98 avg=3.00\n",
            "[202 | 113.48] loss=3.11 avg=3.01\n",
            "[203 | 113.92] loss=2.83 avg=3.00\n",
            "[204 | 114.36] loss=2.70 avg=3.00\n",
            "[205 | 114.81] loss=3.01 avg=3.00\n",
            "[206 | 115.25] loss=2.33 avg=2.99\n",
            "[207 | 115.69] loss=2.69 avg=2.99\n",
            "[208 | 116.13] loss=2.88 avg=2.99\n",
            "[209 | 116.57] loss=2.93 avg=2.99\n",
            "[210 | 117.01] loss=2.89 avg=2.99\n",
            "[211 | 117.46] loss=2.86 avg=2.98\n",
            "[212 | 117.90] loss=3.07 avg=2.99\n",
            "[213 | 118.34] loss=2.79 avg=2.98\n",
            "[214 | 118.78] loss=2.98 avg=2.98\n",
            "[215 | 119.22] loss=3.11 avg=2.98\n",
            "[216 | 119.67] loss=2.94 avg=2.98\n",
            "[217 | 120.11] loss=3.18 avg=2.99\n",
            "[218 | 120.55] loss=3.00 avg=2.99\n",
            "[219 | 120.99] loss=2.99 avg=2.99\n",
            "[220 | 121.44] loss=3.16 avg=2.99\n",
            "[221 | 121.88] loss=3.19 avg=2.99\n",
            "[222 | 122.32] loss=3.07 avg=2.99\n",
            "[223 | 122.76] loss=2.94 avg=2.99\n",
            "[224 | 123.21] loss=3.07 avg=2.99\n",
            "[225 | 123.65] loss=3.06 avg=2.99\n",
            "[226 | 124.09] loss=3.07 avg=2.99\n",
            "[227 | 124.54] loss=3.24 avg=3.00\n",
            "[228 | 124.98] loss=2.98 avg=3.00\n",
            "[229 | 125.42] loss=2.90 avg=3.00\n",
            "[230 | 125.87] loss=2.94 avg=2.99\n",
            "[231 | 126.31] loss=2.88 avg=2.99\n",
            "[232 | 126.76] loss=2.86 avg=2.99\n",
            "[233 | 127.20] loss=2.99 avg=2.99\n",
            "[234 | 127.65] loss=3.01 avg=2.99\n",
            "[235 | 128.09] loss=0.61 avg=2.97\n",
            "[236 | 128.53] loss=3.00 avg=2.97\n",
            "[237 | 128.98] loss=2.81 avg=2.96\n",
            "[238 | 129.42] loss=3.14 avg=2.97\n",
            "[239 | 129.87] loss=3.10 avg=2.97\n",
            "[240 | 130.31] loss=3.03 avg=2.97\n",
            "[241 | 130.76] loss=3.31 avg=2.97\n",
            "[242 | 131.20] loss=2.97 avg=2.97\n",
            "[243 | 131.64] loss=2.85 avg=2.97\n",
            "[244 | 132.09] loss=2.85 avg=2.97\n",
            "[245 | 132.53] loss=3.13 avg=2.97\n",
            "[246 | 132.98] loss=2.84 avg=2.97\n",
            "[247 | 133.42] loss=2.74 avg=2.97\n",
            "[248 | 133.87] loss=3.17 avg=2.97\n",
            "[249 | 134.31] loss=2.99 avg=2.97\n",
            "[250 | 134.76] loss=3.11 avg=2.97\n",
            "[251 | 135.21] loss=2.77 avg=2.97\n",
            "[252 | 135.66] loss=2.78 avg=2.97\n",
            "[253 | 136.10] loss=2.93 avg=2.97\n",
            "[254 | 136.55] loss=3.27 avg=2.97\n",
            "[255 | 136.99] loss=2.91 avg=2.97\n",
            "[256 | 137.44] loss=1.59 avg=2.95\n",
            "[257 | 137.89] loss=3.18 avg=2.96\n",
            "[258 | 138.34] loss=3.17 avg=2.96\n",
            "[259 | 138.78] loss=2.96 avg=2.96\n",
            "[260 | 139.23] loss=2.99 avg=2.96\n",
            "[261 | 139.68] loss=2.90 avg=2.96\n",
            "[262 | 140.12] loss=3.20 avg=2.96\n",
            "[263 | 140.57] loss=2.97 avg=2.96\n",
            "[264 | 141.01] loss=3.02 avg=2.96\n",
            "[265 | 141.46] loss=3.43 avg=2.97\n",
            "[266 | 141.91] loss=3.02 avg=2.97\n",
            "[267 | 142.35] loss=3.21 avg=2.97\n",
            "[268 | 142.80] loss=2.37 avg=2.96\n",
            "[269 | 143.25] loss=3.19 avg=2.97\n",
            "[270 | 143.69] loss=2.95 avg=2.97\n",
            "[271 | 144.14] loss=2.79 avg=2.96\n",
            "[272 | 144.59] loss=2.73 avg=2.96\n",
            "[273 | 145.04] loss=2.17 avg=2.95\n",
            "[274 | 145.49] loss=3.09 avg=2.95\n",
            "[275 | 145.93] loss=2.68 avg=2.95\n",
            "[276 | 146.38] loss=3.01 avg=2.95\n",
            "[277 | 146.82] loss=3.02 avg=2.95\n",
            "[278 | 147.27] loss=2.76 avg=2.95\n",
            "[279 | 147.71] loss=3.02 avg=2.95\n",
            "[280 | 148.16] loss=2.92 avg=2.95\n",
            "[281 | 148.61] loss=0.81 avg=2.93\n",
            "[282 | 149.06] loss=2.90 avg=2.93\n",
            "[283 | 149.51] loss=3.10 avg=2.93\n",
            "[284 | 149.95] loss=2.77 avg=2.93\n",
            "[285 | 150.40] loss=2.59 avg=2.93\n",
            "[286 | 150.85] loss=3.25 avg=2.93\n",
            "[287 | 151.29] loss=2.88 avg=2.93\n",
            "[288 | 151.74] loss=2.92 avg=2.93\n",
            "[289 | 152.19] loss=3.09 avg=2.93\n",
            "[290 | 152.63] loss=2.76 avg=2.93\n",
            "[291 | 153.08] loss=2.89 avg=2.93\n",
            "[292 | 153.53] loss=2.96 avg=2.93\n",
            "[293 | 153.97] loss=3.08 avg=2.93\n",
            "[294 | 154.42] loss=2.96 avg=2.93\n",
            "[295 | 154.87] loss=2.82 avg=2.93\n",
            "[296 | 155.32] loss=2.57 avg=2.92\n",
            "[297 | 155.76] loss=3.00 avg=2.93\n",
            "[298 | 156.21] loss=2.99 avg=2.93\n",
            "[299 | 156.66] loss=3.15 avg=2.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " ph.\n",
            "\n",
            "The question is, how can we know that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and that the same thing is not the same thing, and\n",
            "\n",
            "[300 | 167.27] loss=2.73 avg=2.93\n",
            "[301 | 167.72] loss=2.88 avg=2.93\n",
            "[302 | 168.16] loss=3.19 avg=2.93\n",
            "[303 | 168.61] loss=3.02 avg=2.93\n",
            "[304 | 169.06] loss=3.16 avg=2.93\n",
            "[305 | 169.50] loss=3.13 avg=2.93\n",
            "[306 | 169.95] loss=3.10 avg=2.94\n",
            "[307 | 170.40] loss=2.70 avg=2.93\n",
            "[308 | 170.84] loss=3.37 avg=2.94\n",
            "[309 | 171.29] loss=3.27 avg=2.94\n",
            "[310 | 171.74] loss=2.79 avg=2.94\n",
            "[311 | 172.19] loss=2.21 avg=2.93\n",
            "[312 | 172.63] loss=2.42 avg=2.93\n",
            "[313 | 173.08] loss=3.19 avg=2.93\n",
            "[314 | 173.52] loss=2.86 avg=2.93\n",
            "[315 | 173.97] loss=3.24 avg=2.93\n",
            "[316 | 174.42] loss=3.12 avg=2.93\n",
            "[317 | 174.86] loss=2.90 avg=2.93\n",
            "[318 | 175.31] loss=2.95 avg=2.93\n",
            "[319 | 175.76] loss=3.12 avg=2.94\n",
            "[320 | 176.20] loss=3.06 avg=2.94\n",
            "[321 | 176.65] loss=3.10 avg=2.94\n",
            "[322 | 177.10] loss=3.00 avg=2.94\n",
            "[323 | 177.54] loss=2.98 avg=2.94\n",
            "[324 | 177.99] loss=2.86 avg=2.94\n",
            "[325 | 178.43] loss=2.76 avg=2.94\n",
            "[326 | 178.88] loss=2.92 avg=2.94\n",
            "[327 | 179.33] loss=3.13 avg=2.94\n",
            "[328 | 179.77] loss=3.04 avg=2.94\n",
            "[329 | 180.22] loss=3.02 avg=2.94\n",
            "[330 | 180.67] loss=2.94 avg=2.94\n",
            "[331 | 181.12] loss=2.89 avg=2.94\n",
            "[332 | 181.56] loss=3.22 avg=2.94\n",
            "[333 | 182.01] loss=3.06 avg=2.94\n",
            "[334 | 182.46] loss=3.06 avg=2.95\n",
            "[335 | 182.89] loss=3.06 avg=2.95\n",
            "[336 | 183.34] loss=3.07 avg=2.95\n",
            "[337 | 183.79] loss=3.27 avg=2.95\n",
            "[338 | 184.24] loss=2.11 avg=2.94\n",
            "[339 | 184.69] loss=2.82 avg=2.94\n",
            "[340 | 185.14] loss=2.91 avg=2.94\n",
            "[341 | 185.59] loss=2.85 avg=2.94\n",
            "[342 | 186.04] loss=0.90 avg=2.92\n",
            "[343 | 186.49] loss=3.22 avg=2.92\n",
            "[344 | 186.94] loss=3.11 avg=2.92\n",
            "[345 | 187.39] loss=2.94 avg=2.92\n",
            "[346 | 187.83] loss=2.72 avg=2.92\n",
            "[347 | 188.28] loss=3.42 avg=2.93\n",
            "[348 | 188.73] loss=1.94 avg=2.92\n",
            "[349 | 189.18] loss=2.80 avg=2.92\n",
            "[350 | 189.63] loss=2.85 avg=2.92\n",
            "[351 | 190.08] loss=2.99 avg=2.92\n",
            "[352 | 190.53] loss=2.99 avg=2.92\n",
            "[353 | 190.98] loss=2.82 avg=2.92\n",
            "[354 | 191.43] loss=2.84 avg=2.92\n",
            "[355 | 191.87] loss=2.85 avg=2.91\n",
            "[356 | 192.32] loss=2.87 avg=2.91\n",
            "[357 | 192.77] loss=2.98 avg=2.91\n",
            "[358 | 193.22] loss=2.89 avg=2.91\n",
            "[359 | 193.67] loss=3.05 avg=2.92\n",
            "[360 | 194.13] loss=3.06 avg=2.92\n",
            "[361 | 194.57] loss=3.12 avg=2.92\n",
            "[362 | 195.02] loss=3.13 avg=2.92\n",
            "[363 | 195.47] loss=3.11 avg=2.92\n",
            "[364 | 195.92] loss=2.84 avg=2.92\n",
            "[365 | 196.36] loss=3.03 avg=2.92\n",
            "[366 | 196.81] loss=3.11 avg=2.93\n",
            "[367 | 197.26] loss=2.88 avg=2.93\n",
            "[368 | 197.71] loss=2.86 avg=2.92\n",
            "[369 | 198.16] loss=2.81 avg=2.92\n",
            "[370 | 198.61] loss=2.76 avg=2.92\n",
            "[371 | 199.05] loss=2.94 avg=2.92\n",
            "[372 | 199.50] loss=2.88 avg=2.92\n",
            "[373 | 199.96] loss=2.99 avg=2.92\n",
            "[374 | 200.40] loss=2.83 avg=2.92\n",
            "[375 | 200.85] loss=3.11 avg=2.92\n",
            "[376 | 201.30] loss=3.11 avg=2.93\n",
            "[377 | 201.75] loss=3.06 avg=2.93\n",
            "[378 | 202.21] loss=3.46 avg=2.93\n",
            "[379 | 202.65] loss=2.77 avg=2.93\n",
            "[380 | 203.10] loss=3.03 avg=2.93\n",
            "[381 | 203.55] loss=3.15 avg=2.93\n",
            "[382 | 204.00] loss=2.74 avg=2.93\n",
            "[383 | 204.45] loss=3.22 avg=2.93\n",
            "[384 | 204.90] loss=3.11 avg=2.94\n",
            "[385 | 205.35] loss=2.86 avg=2.94\n",
            "[386 | 205.80] loss=3.10 avg=2.94\n",
            "[387 | 206.25] loss=2.82 avg=2.94\n",
            "[388 | 206.70] loss=3.08 avg=2.94\n",
            "[389 | 207.15] loss=2.78 avg=2.94\n",
            "[390 | 207.60] loss=2.93 avg=2.94\n",
            "[391 | 208.04] loss=3.17 avg=2.94\n",
            "[392 | 208.50] loss=3.04 avg=2.94\n",
            "[393 | 208.95] loss=2.93 avg=2.94\n",
            "[394 | 209.40] loss=2.77 avg=2.94\n",
            "[395 | 209.85] loss=2.96 avg=2.94\n",
            "[396 | 210.30] loss=2.83 avg=2.94\n",
            "[397 | 210.75] loss=3.02 avg=2.94\n",
            "[398 | 211.20] loss=2.81 avg=2.94\n",
            "[399 | 211.65] loss=3.22 avg=2.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\n",
            "The problem is that the problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things, but a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world. For the world is not a world, but a world of things, and the world is not a world of things.\n",
            "\n",
            "The problem of the existence of a world is not a problem of the existence of a world, but of the existence of a world\n",
            "\n",
            "[400 | 222.33] loss=2.79 avg=2.94\n",
            "[401 | 222.79] loss=3.11 avg=2.94\n",
            "[402 | 223.24] loss=2.65 avg=2.94\n",
            "[403 | 223.69] loss=2.53 avg=2.93\n",
            "[404 | 224.14] loss=3.08 avg=2.93\n",
            "[405 | 224.59] loss=2.96 avg=2.93\n",
            "[406 | 225.04] loss=1.71 avg=2.92\n",
            "[407 | 225.49] loss=2.95 avg=2.92\n",
            "[408 | 225.94] loss=3.11 avg=2.92\n",
            "[409 | 226.38] loss=1.55 avg=2.91\n",
            "[410 | 226.83] loss=2.69 avg=2.91\n",
            "[411 | 227.28] loss=3.22 avg=2.91\n",
            "[412 | 227.73] loss=2.86 avg=2.91\n",
            "[413 | 228.18] loss=2.94 avg=2.91\n",
            "[414 | 228.63] loss=2.71 avg=2.91\n",
            "[415 | 229.09] loss=3.15 avg=2.91\n",
            "[416 | 229.54] loss=2.89 avg=2.91\n",
            "[417 | 229.98] loss=2.67 avg=2.91\n",
            "[418 | 230.44] loss=2.73 avg=2.91\n",
            "[419 | 230.89] loss=3.02 avg=2.91\n",
            "[420 | 231.34] loss=2.93 avg=2.91\n",
            "[421 | 231.80] loss=2.60 avg=2.90\n",
            "[422 | 232.25] loss=2.76 avg=2.90\n",
            "[423 | 232.70] loss=2.95 avg=2.90\n",
            "[424 | 233.15] loss=2.83 avg=2.90\n",
            "[425 | 233.61] loss=2.92 avg=2.90\n",
            "[426 | 234.06] loss=2.59 avg=2.90\n",
            "[427 | 234.50] loss=2.87 avg=2.90\n",
            "[428 | 234.95] loss=2.89 avg=2.90\n",
            "[429 | 235.40] loss=2.84 avg=2.90\n",
            "[430 | 235.85] loss=2.96 avg=2.90\n",
            "[431 | 236.30] loss=3.24 avg=2.90\n",
            "[432 | 236.75] loss=3.18 avg=2.91\n",
            "[433 | 237.21] loss=2.50 avg=2.90\n",
            "[434 | 237.66] loss=2.88 avg=2.90\n",
            "[435 | 238.11] loss=2.73 avg=2.90\n",
            "[436 | 238.56] loss=2.86 avg=2.90\n",
            "[437 | 239.01] loss=3.10 avg=2.90\n",
            "[438 | 239.46] loss=2.72 avg=2.90\n",
            "[439 | 239.92] loss=3.14 avg=2.90\n",
            "[440 | 240.37] loss=2.66 avg=2.90\n",
            "[441 | 240.82] loss=2.80 avg=2.90\n",
            "[442 | 241.27] loss=2.59 avg=2.90\n",
            "[443 | 241.72] loss=3.14 avg=2.90\n",
            "[444 | 242.17] loss=2.81 avg=2.90\n",
            "[445 | 242.63] loss=2.77 avg=2.90\n",
            "[446 | 243.08] loss=2.96 avg=2.90\n",
            "[447 | 243.53] loss=3.06 avg=2.90\n",
            "[448 | 243.98] loss=2.96 avg=2.90\n",
            "[449 | 244.44] loss=0.98 avg=2.88\n",
            "[450 | 244.89] loss=2.83 avg=2.88\n",
            "[451 | 245.34] loss=2.74 avg=2.88\n",
            "[452 | 245.79] loss=2.78 avg=2.88\n",
            "[453 | 246.24] loss=2.73 avg=2.88\n",
            "[454 | 246.70] loss=3.04 avg=2.88\n",
            "[455 | 247.15] loss=2.75 avg=2.88\n",
            "[456 | 247.60] loss=2.89 avg=2.88\n",
            "[457 | 248.05] loss=2.80 avg=2.87\n",
            "[458 | 248.51] loss=2.74 avg=2.87\n",
            "[459 | 248.96] loss=2.85 avg=2.87\n",
            "[460 | 249.41] loss=2.87 avg=2.87\n",
            "[461 | 249.87] loss=3.02 avg=2.87\n",
            "[462 | 250.32] loss=2.74 avg=2.87\n",
            "[463 | 250.77] loss=2.95 avg=2.87\n",
            "[464 | 251.23] loss=2.77 avg=2.87\n",
            "[465 | 251.68] loss=2.91 avg=2.87\n",
            "[466 | 252.13] loss=3.79 avg=2.88\n",
            "[467 | 252.58] loss=2.68 avg=2.88\n",
            "[468 | 253.03] loss=2.89 avg=2.88\n",
            "[469 | 253.48] loss=2.83 avg=2.88\n",
            "[470 | 253.94] loss=2.66 avg=2.88\n",
            "[471 | 254.39] loss=2.83 avg=2.88\n",
            "[472 | 254.84] loss=2.79 avg=2.88\n",
            "[473 | 255.30] loss=2.90 avg=2.88\n",
            "[474 | 255.75] loss=2.79 avg=2.88\n",
            "[475 | 256.21] loss=2.93 avg=2.88\n",
            "[476 | 256.66] loss=2.97 avg=2.88\n",
            "[477 | 257.11] loss=2.84 avg=2.88\n",
            "[478 | 257.56] loss=2.68 avg=2.88\n",
            "[479 | 258.02] loss=2.99 avg=2.88\n",
            "[480 | 258.47] loss=2.70 avg=2.87\n",
            "[481 | 258.93] loss=3.26 avg=2.88\n",
            "[482 | 259.38] loss=3.02 avg=2.88\n",
            "[483 | 259.83] loss=2.67 avg=2.88\n",
            "[484 | 260.28] loss=2.90 avg=2.88\n",
            "[485 | 260.74] loss=2.65 avg=2.88\n",
            "[486 | 261.19] loss=3.09 avg=2.88\n",
            "[487 | 261.65] loss=2.87 avg=2.88\n",
            "[488 | 262.10] loss=3.18 avg=2.88\n",
            "[489 | 262.55] loss=2.59 avg=2.88\n",
            "[490 | 263.00] loss=2.70 avg=2.88\n",
            "[491 | 263.45] loss=3.01 avg=2.88\n",
            "[492 | 263.91] loss=2.89 avg=2.88\n",
            "[493 | 264.37] loss=2.98 avg=2.88\n",
            "[494 | 264.82] loss=2.98 avg=2.88\n",
            "[495 | 265.27] loss=3.12 avg=2.88\n",
            "[496 | 265.73] loss=2.69 avg=2.88\n",
            "[497 | 266.18] loss=3.08 avg=2.88\n",
            "[498 | 266.63] loss=2.89 avg=2.88\n",
            "[499 | 267.08] loss=3.13 avg=2.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\n",
            "The first step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The second step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The third step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The fourth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The fifth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The sixth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The seventh step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The eighth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The ninth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The tenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The eleventh step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The twelfth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the synthesis of the two parts of the body.\n",
            "\n",
            "The thirteenth step in the process of the synthesis of the two parts of the soul is the\n",
            "\n",
            "[500 | 277.72] loss=2.74 avg=2.88\n",
            "[501 | 278.19] loss=2.97 avg=2.88\n",
            "[502 | 278.64] loss=1.20 avg=2.87\n",
            "[503 | 279.09] loss=2.75 avg=2.87\n",
            "[504 | 279.55] loss=2.64 avg=2.86\n",
            "[505 | 280.00] loss=3.13 avg=2.87\n",
            "[506 | 280.46] loss=2.44 avg=2.86\n",
            "[507 | 280.91] loss=2.78 avg=2.86\n",
            "[508 | 281.36] loss=2.95 avg=2.86\n",
            "[509 | 281.82] loss=3.03 avg=2.86\n",
            "[510 | 282.27] loss=2.67 avg=2.86\n",
            "[511 | 282.73] loss=2.76 avg=2.86\n",
            "[512 | 283.18] loss=2.96 avg=2.86\n",
            "[513 | 283.63] loss=3.26 avg=2.87\n",
            "[514 | 284.08] loss=2.78 avg=2.86\n",
            "[515 | 284.54] loss=2.99 avg=2.87\n",
            "[516 | 285.00] loss=3.17 avg=2.87\n",
            "[517 | 285.45] loss=3.04 avg=2.87\n",
            "[518 | 285.90] loss=3.06 avg=2.87\n",
            "[519 | 286.36] loss=3.27 avg=2.88\n",
            "[520 | 286.81] loss=3.19 avg=2.88\n",
            "[521 | 287.27] loss=2.58 avg=2.88\n",
            "[522 | 287.72] loss=2.93 avg=2.88\n",
            "[523 | 288.17] loss=2.78 avg=2.88\n",
            "[524 | 288.63] loss=3.01 avg=2.88\n",
            "[525 | 289.08] loss=2.95 avg=2.88\n",
            "[526 | 289.53] loss=2.75 avg=2.88\n",
            "[527 | 289.99] loss=0.51 avg=2.85\n",
            "[528 | 290.44] loss=3.06 avg=2.86\n",
            "[529 | 290.89] loss=2.78 avg=2.85\n",
            "[530 | 291.35] loss=3.14 avg=2.86\n",
            "[531 | 291.80] loss=3.01 avg=2.86\n",
            "[532 | 292.26] loss=2.87 avg=2.86\n",
            "[533 | 292.71] loss=3.05 avg=2.86\n",
            "[534 | 293.17] loss=2.73 avg=2.86\n",
            "[535 | 293.62] loss=2.88 avg=2.86\n",
            "[536 | 294.08] loss=3.04 avg=2.86\n",
            "[537 | 294.54] loss=3.24 avg=2.87\n",
            "[538 | 295.00] loss=3.04 avg=2.87\n",
            "[539 | 295.45] loss=2.70 avg=2.87\n",
            "[540 | 295.90] loss=2.79 avg=2.87\n",
            "[541 | 296.36] loss=2.95 avg=2.87\n",
            "[542 | 296.81] loss=1.94 avg=2.86\n",
            "[543 | 297.27] loss=3.03 avg=2.86\n",
            "[544 | 297.72] loss=2.92 avg=2.86\n",
            "[545 | 298.18] loss=3.30 avg=2.86\n",
            "[546 | 298.63] loss=3.16 avg=2.87\n",
            "[547 | 299.08] loss=2.79 avg=2.87\n",
            "[548 | 299.54] loss=2.89 avg=2.87\n",
            "[549 | 299.99] loss=3.04 avg=2.87\n",
            "[550 | 300.45] loss=2.86 avg=2.87\n",
            "[551 | 300.91] loss=2.93 avg=2.87\n",
            "[552 | 301.37] loss=3.00 avg=2.87\n",
            "[553 | 301.82] loss=0.66 avg=2.85\n",
            "[554 | 302.28] loss=2.81 avg=2.85\n",
            "[555 | 302.73] loss=2.93 avg=2.85\n",
            "[556 | 303.19] loss=2.76 avg=2.85\n",
            "[557 | 303.64] loss=3.15 avg=2.85\n",
            "[558 | 304.10] loss=3.22 avg=2.85\n",
            "[559 | 304.55] loss=3.00 avg=2.86\n",
            "[560 | 305.01] loss=2.18 avg=2.85\n",
            "[561 | 305.46] loss=2.60 avg=2.85\n",
            "[562 | 305.92] loss=2.92 avg=2.85\n",
            "[563 | 306.37] loss=3.02 avg=2.85\n",
            "[564 | 306.83] loss=3.00 avg=2.85\n",
            "[565 | 307.28] loss=2.88 avg=2.85\n",
            "[566 | 307.74] loss=2.56 avg=2.85\n",
            "[567 | 308.19] loss=0.73 avg=2.83\n",
            "[568 | 308.65] loss=2.83 avg=2.83\n",
            "[569 | 309.10] loss=3.17 avg=2.83\n",
            "[570 | 309.56] loss=2.66 avg=2.83\n",
            "[571 | 310.01] loss=2.66 avg=2.83\n",
            "[572 | 310.47] loss=2.31 avg=2.82\n",
            "[573 | 310.92] loss=2.89 avg=2.82\n",
            "[574 | 311.38] loss=2.65 avg=2.82\n",
            "[575 | 311.83] loss=2.76 avg=2.82\n",
            "[576 | 312.28] loss=3.00 avg=2.82\n",
            "[577 | 312.74] loss=2.70 avg=2.82\n",
            "[578 | 313.19] loss=2.93 avg=2.82\n",
            "[579 | 313.65] loss=2.90 avg=2.82\n",
            "[580 | 314.10] loss=2.71 avg=2.82\n",
            "[581 | 314.56] loss=2.69 avg=2.82\n",
            "[582 | 315.02] loss=3.10 avg=2.82\n",
            "[583 | 315.47] loss=2.85 avg=2.82\n",
            "[584 | 315.93] loss=2.97 avg=2.82\n",
            "[585 | 316.38] loss=2.72 avg=2.82\n",
            "[586 | 316.84] loss=3.04 avg=2.83\n",
            "[587 | 317.29] loss=3.06 avg=2.83\n",
            "[588 | 317.74] loss=1.16 avg=2.81\n",
            "[589 | 318.20] loss=2.95 avg=2.81\n",
            "[590 | 318.65] loss=2.38 avg=2.81\n",
            "[591 | 319.12] loss=0.48 avg=2.78\n",
            "[592 | 319.57] loss=2.87 avg=2.79\n",
            "[593 | 320.02] loss=3.25 avg=2.79\n",
            "[594 | 320.48] loss=3.07 avg=2.79\n",
            "[595 | 320.93] loss=2.72 avg=2.79\n",
            "[596 | 321.39] loss=0.39 avg=2.77\n",
            "[597 | 321.84] loss=2.94 avg=2.77\n",
            "[598 | 322.30] loss=2.90 avg=2.77\n",
            "[599 | 322.75] loss=3.33 avg=2.78\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " which is to say, that the whole of the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion, and that the world is a mere illusion,\n",
            "\n",
            "[600 | 333.46] loss=2.69 avg=2.78\n",
            "[601 | 333.93] loss=2.73 avg=2.78\n",
            "[602 | 334.39] loss=2.82 avg=2.78\n",
            "[603 | 334.83] loss=2.68 avg=2.77\n",
            "[604 | 335.29] loss=2.63 avg=2.77\n",
            "[605 | 335.75] loss=2.78 avg=2.77\n",
            "[606 | 336.20] loss=3.08 avg=2.78\n",
            "[607 | 336.66] loss=2.78 avg=2.78\n",
            "[608 | 337.11] loss=3.10 avg=2.78\n",
            "[609 | 337.57] loss=3.02 avg=2.78\n",
            "[610 | 338.03] loss=3.05 avg=2.78\n",
            "[611 | 338.48] loss=2.75 avg=2.78\n",
            "[612 | 338.93] loss=2.66 avg=2.78\n",
            "[613 | 339.39] loss=2.67 avg=2.78\n",
            "[614 | 339.85] loss=2.78 avg=2.78\n",
            "[615 | 340.30] loss=2.69 avg=2.78\n",
            "[616 | 340.76] loss=2.51 avg=2.78\n",
            "[617 | 341.21] loss=2.88 avg=2.78\n",
            "[618 | 341.66] loss=2.74 avg=2.78\n",
            "[619 | 342.12] loss=2.75 avg=2.78\n",
            "[620 | 342.57] loss=2.61 avg=2.78\n",
            "[621 | 343.03] loss=2.76 avg=2.78\n",
            "[622 | 343.49] loss=2.68 avg=2.78\n",
            "[623 | 343.94] loss=2.67 avg=2.77\n",
            "[624 | 344.40] loss=2.98 avg=2.78\n",
            "[625 | 344.85] loss=3.21 avg=2.78\n",
            "[626 | 345.31] loss=2.64 avg=2.78\n",
            "[627 | 345.77] loss=1.55 avg=2.77\n",
            "[628 | 346.23] loss=2.47 avg=2.76\n",
            "[629 | 346.68] loss=2.57 avg=2.76\n",
            "[630 | 347.14] loss=2.90 avg=2.76\n",
            "[631 | 347.59] loss=2.70 avg=2.76\n",
            "[632 | 348.05] loss=3.20 avg=2.77\n",
            "[633 | 348.50] loss=3.02 avg=2.77\n",
            "[634 | 348.96] loss=2.95 avg=2.77\n",
            "[635 | 349.41] loss=2.70 avg=2.77\n",
            "[636 | 349.87] loss=2.90 avg=2.77\n",
            "[637 | 350.32] loss=3.12 avg=2.78\n",
            "[638 | 350.78] loss=2.51 avg=2.77\n",
            "[639 | 351.23] loss=2.61 avg=2.77\n",
            "[640 | 351.69] loss=2.77 avg=2.77\n",
            "[641 | 352.14] loss=2.99 avg=2.77\n",
            "[642 | 352.60] loss=2.99 avg=2.78\n",
            "[643 | 353.05] loss=2.87 avg=2.78\n",
            "[644 | 353.51] loss=3.31 avg=2.78\n",
            "[645 | 353.96] loss=3.27 avg=2.79\n",
            "[646 | 354.42] loss=2.71 avg=2.79\n",
            "[647 | 354.88] loss=3.25 avg=2.79\n",
            "[648 | 355.34] loss=2.93 avg=2.79\n",
            "[649 | 355.79] loss=2.94 avg=2.79\n",
            "[650 | 356.24] loss=3.01 avg=2.80\n",
            "[651 | 356.70] loss=3.07 avg=2.80\n",
            "[652 | 357.16] loss=2.77 avg=2.80\n",
            "[653 | 357.61] loss=2.93 avg=2.80\n",
            "[654 | 358.07] loss=2.96 avg=2.80\n",
            "[655 | 358.52] loss=2.08 avg=2.79\n",
            "[656 | 358.98] loss=2.91 avg=2.80\n",
            "[657 | 359.43] loss=2.73 avg=2.79\n",
            "[658 | 359.89] loss=0.39 avg=2.77\n",
            "[659 | 360.35] loss=2.69 avg=2.77\n",
            "[660 | 360.80] loss=2.74 avg=2.77\n",
            "[661 | 361.26] loss=2.89 avg=2.77\n",
            "[662 | 361.71] loss=2.36 avg=2.77\n",
            "[663 | 362.17] loss=0.33 avg=2.74\n",
            "[664 | 362.62] loss=2.84 avg=2.74\n",
            "[665 | 363.08] loss=2.58 avg=2.74\n",
            "[666 | 363.54] loss=1.22 avg=2.73\n",
            "[667 | 363.99] loss=3.21 avg=2.73\n",
            "[668 | 364.45] loss=2.98 avg=2.73\n",
            "[669 | 364.90] loss=2.70 avg=2.73\n",
            "[670 | 365.36] loss=2.85 avg=2.73\n",
            "[671 | 365.82] loss=2.87 avg=2.74\n",
            "[672 | 366.27] loss=1.86 avg=2.73\n",
            "[673 | 366.73] loss=2.11 avg=2.72\n",
            "[674 | 367.18] loss=2.82 avg=2.72\n",
            "[675 | 367.64] loss=1.92 avg=2.71\n",
            "[676 | 368.10] loss=2.87 avg=2.72\n",
            "[677 | 368.56] loss=2.91 avg=2.72\n",
            "[678 | 369.01] loss=2.47 avg=2.72\n",
            "[679 | 369.47] loss=3.29 avg=2.72\n",
            "[680 | 369.93] loss=1.21 avg=2.71\n",
            "[681 | 370.38] loss=2.94 avg=2.71\n",
            "[682 | 370.84] loss=3.15 avg=2.71\n",
            "[683 | 371.29] loss=3.00 avg=2.72\n",
            "[684 | 371.75] loss=2.71 avg=2.72\n",
            "[685 | 372.20] loss=2.73 avg=2.72\n",
            "[686 | 372.66] loss=3.15 avg=2.72\n",
            "[687 | 373.12] loss=2.59 avg=2.72\n",
            "[688 | 373.58] loss=2.87 avg=2.72\n",
            "[689 | 374.03] loss=3.00 avg=2.72\n",
            "[690 | 374.49] loss=3.15 avg=2.73\n",
            "[691 | 374.95] loss=2.62 avg=2.73\n",
            "[692 | 375.41] loss=2.67 avg=2.73\n",
            "[693 | 375.86] loss=2.65 avg=2.72\n",
            "[694 | 376.32] loss=2.69 avg=2.72\n",
            "[695 | 376.78] loss=2.97 avg=2.73\n",
            "[696 | 377.24] loss=2.38 avg=2.72\n",
            "[697 | 377.70] loss=2.85 avg=2.72\n",
            "[698 | 378.15] loss=2.91 avg=2.73\n",
            "[699 | 378.61] loss=2.74 avg=2.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " that the law of nature is not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature, and that the law of nature is not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "The law of nature is, therefore, not to be regarded as a law of nature, but as a law of nature.\n",
            "\n",
            "\n",
            "\n",
            "[700 | 389.32] loss=2.76 avg=2.73\n",
            "[701 | 389.79] loss=2.84 avg=2.73\n",
            "[702 | 390.24] loss=2.96 avg=2.73\n",
            "[703 | 390.69] loss=2.89 avg=2.73\n",
            "[704 | 391.15] loss=2.95 avg=2.73\n",
            "[705 | 391.60] loss=2.88 avg=2.74\n",
            "[706 | 392.06] loss=2.60 avg=2.73\n",
            "[707 | 392.51] loss=2.76 avg=2.73\n",
            "[708 | 392.97] loss=1.85 avg=2.73\n",
            "[709 | 393.43] loss=2.64 avg=2.73\n",
            "[710 | 393.89] loss=3.14 avg=2.73\n",
            "[711 | 394.34] loss=3.09 avg=2.73\n",
            "[712 | 394.80] loss=3.02 avg=2.74\n",
            "[713 | 395.26] loss=2.73 avg=2.74\n",
            "[714 | 395.71] loss=2.91 avg=2.74\n",
            "[715 | 396.17] loss=2.85 avg=2.74\n",
            "[716 | 396.62] loss=2.97 avg=2.74\n",
            "[717 | 397.08] loss=2.72 avg=2.74\n",
            "[718 | 397.54] loss=3.03 avg=2.74\n",
            "[719 | 398.00] loss=2.44 avg=2.74\n",
            "[720 | 398.45] loss=2.62 avg=2.74\n",
            "[721 | 398.91] loss=2.34 avg=2.74\n",
            "[722 | 399.37] loss=2.75 avg=2.74\n",
            "[723 | 399.82] loss=3.04 avg=2.74\n",
            "[724 | 400.28] loss=2.62 avg=2.74\n",
            "[725 | 400.74] loss=2.57 avg=2.74\n",
            "[726 | 401.20] loss=2.65 avg=2.73\n",
            "[727 | 401.66] loss=2.57 avg=2.73\n",
            "[728 | 402.12] loss=2.96 avg=2.74\n",
            "[729 | 402.57] loss=2.81 avg=2.74\n",
            "[730 | 403.03] loss=2.94 avg=2.74\n",
            "[731 | 403.48] loss=2.78 avg=2.74\n",
            "[732 | 403.94] loss=3.22 avg=2.74\n",
            "[733 | 404.40] loss=2.89 avg=2.74\n",
            "[734 | 404.85] loss=3.15 avg=2.75\n",
            "[735 | 405.31] loss=2.72 avg=2.75\n",
            "[736 | 405.77] loss=2.79 avg=2.75\n",
            "[737 | 406.22] loss=2.82 avg=2.75\n",
            "[738 | 406.68] loss=2.70 avg=2.75\n",
            "[739 | 407.14] loss=2.66 avg=2.75\n",
            "[740 | 407.59] loss=2.69 avg=2.75\n",
            "[741 | 408.05] loss=2.88 avg=2.75\n",
            "[742 | 408.51] loss=0.42 avg=2.73\n",
            "[743 | 408.97] loss=3.04 avg=2.73\n",
            "[744 | 409.43] loss=2.99 avg=2.73\n",
            "[745 | 409.88] loss=3.05 avg=2.73\n",
            "[746 | 410.33] loss=2.64 avg=2.73\n",
            "[747 | 410.79] loss=2.89 avg=2.74\n",
            "[748 | 411.25] loss=2.76 avg=2.74\n",
            "[749 | 411.70] loss=2.79 avg=2.74\n",
            "[750 | 412.16] loss=2.58 avg=2.73\n",
            "[751 | 412.61] loss=2.75 avg=2.73\n",
            "[752 | 413.07] loss=2.81 avg=2.74\n",
            "[753 | 413.52] loss=2.98 avg=2.74\n",
            "[754 | 413.98] loss=2.96 avg=2.74\n",
            "[755 | 414.44] loss=2.81 avg=2.74\n",
            "[756 | 414.90] loss=2.91 avg=2.74\n",
            "[757 | 415.35] loss=2.62 avg=2.74\n",
            "[758 | 415.81] loss=2.96 avg=2.74\n",
            "[759 | 416.27] loss=2.57 avg=2.74\n",
            "[760 | 416.72] loss=2.72 avg=2.74\n",
            "[761 | 417.18] loss=2.58 avg=2.74\n",
            "[762 | 417.64] loss=0.62 avg=2.72\n",
            "[763 | 418.10] loss=2.30 avg=2.71\n",
            "[764 | 418.55] loss=2.62 avg=2.71\n",
            "[765 | 419.01] loss=3.03 avg=2.72\n",
            "[766 | 419.47] loss=2.52 avg=2.71\n",
            "[767 | 419.92] loss=2.91 avg=2.72\n",
            "[768 | 420.38] loss=2.72 avg=2.72\n",
            "[769 | 420.84] loss=2.99 avg=2.72\n",
            "[770 | 421.30] loss=2.85 avg=2.72\n",
            "[771 | 421.76] loss=2.95 avg=2.72\n",
            "[772 | 422.21] loss=2.76 avg=2.72\n",
            "[773 | 422.66] loss=2.66 avg=2.72\n",
            "[774 | 423.12] loss=3.38 avg=2.73\n",
            "[775 | 423.58] loss=2.69 avg=2.73\n",
            "[776 | 424.04] loss=2.16 avg=2.72\n",
            "[777 | 424.49] loss=2.56 avg=2.72\n",
            "[778 | 424.95] loss=2.84 avg=2.72\n",
            "[779 | 425.41] loss=2.70 avg=2.72\n",
            "[780 | 425.86] loss=2.66 avg=2.72\n",
            "[781 | 426.32] loss=2.58 avg=2.72\n",
            "[782 | 426.77] loss=2.90 avg=2.72\n",
            "[783 | 427.23] loss=2.83 avg=2.72\n",
            "[784 | 427.68] loss=3.08 avg=2.73\n",
            "[785 | 428.14] loss=3.22 avg=2.73\n",
            "[786 | 428.59] loss=2.79 avg=2.73\n",
            "[787 | 429.04] loss=2.54 avg=2.73\n",
            "[788 | 429.50] loss=2.65 avg=2.73\n",
            "[789 | 429.96] loss=3.23 avg=2.73\n",
            "[790 | 430.42] loss=3.13 avg=2.74\n",
            "[791 | 430.88] loss=2.68 avg=2.74\n",
            "[792 | 431.34] loss=2.79 avg=2.74\n",
            "[793 | 431.79] loss=2.68 avg=2.74\n",
            "[794 | 432.25] loss=2.98 avg=2.74\n",
            "[795 | 432.71] loss=2.57 avg=2.74\n",
            "[796 | 433.17] loss=2.99 avg=2.74\n",
            "[797 | 433.62] loss=3.28 avg=2.75\n",
            "[798 | 434.08] loss=2.89 avg=2.75\n",
            "[799 | 434.54] loss=0.77 avg=2.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "(2) The term \"conception\" is not to be taken as a term of respect for the conception of a thing, but as a term of respect for the conception of a thing in general.\n",
            "\n",
            "(3) The term \"conception\" is not to be taken as a term of respect for the conception of a thing in general, but as a term of respect for the conception of a thing in general in general.\n",
            "\n",
            "(4) The term \"conception\" is not to be taken as a term of respect for the conception of a thing in general, but as a term of respect for the conception of a thing in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general in general\n",
            "\n",
            "[800 | 445.23] loss=2.78 avg=2.73\n",
            "[801 | 445.71] loss=2.86 avg=2.73\n",
            "[802 | 446.16] loss=3.06 avg=2.73\n",
            "[803 | 446.62] loss=2.91 avg=2.74\n",
            "[804 | 447.08] loss=2.71 avg=2.73\n",
            "[805 | 447.53] loss=2.91 avg=2.74\n",
            "[806 | 447.99] loss=0.26 avg=2.71\n",
            "[807 | 448.45] loss=2.77 avg=2.71\n",
            "[808 | 448.90] loss=2.55 avg=2.71\n",
            "[809 | 449.36] loss=2.98 avg=2.71\n",
            "[810 | 449.82] loss=2.70 avg=2.71\n",
            "[811 | 450.27] loss=3.06 avg=2.72\n",
            "[812 | 450.73] loss=3.08 avg=2.72\n",
            "[813 | 451.19] loss=3.12 avg=2.72\n",
            "[814 | 451.65] loss=3.15 avg=2.73\n",
            "[815 | 452.11] loss=2.96 avg=2.73\n",
            "[816 | 452.56] loss=0.75 avg=2.71\n",
            "[817 | 453.02] loss=2.85 avg=2.71\n",
            "[818 | 453.48] loss=2.82 avg=2.71\n",
            "[819 | 453.93] loss=2.94 avg=2.72\n",
            "[820 | 454.39] loss=1.53 avg=2.70\n",
            "[821 | 454.85] loss=3.09 avg=2.71\n",
            "[822 | 455.30] loss=2.64 avg=2.71\n",
            "[823 | 455.76] loss=2.76 avg=2.71\n",
            "[824 | 456.21] loss=3.47 avg=2.72\n",
            "[825 | 456.67] loss=2.63 avg=2.71\n",
            "[826 | 457.13] loss=2.58 avg=2.71\n",
            "[827 | 457.59] loss=2.63 avg=2.71\n",
            "[828 | 458.05] loss=2.83 avg=2.71\n",
            "[829 | 458.50] loss=2.59 avg=2.71\n",
            "[830 | 458.96] loss=2.65 avg=2.71\n",
            "[831 | 459.42] loss=0.28 avg=2.69\n",
            "[832 | 459.88] loss=3.04 avg=2.69\n",
            "[833 | 460.33] loss=2.64 avg=2.69\n",
            "[834 | 460.79] loss=3.16 avg=2.69\n",
            "[835 | 461.24] loss=1.97 avg=2.69\n",
            "[836 | 461.70] loss=3.30 avg=2.69\n",
            "[837 | 462.16] loss=2.53 avg=2.69\n",
            "[838 | 462.62] loss=2.69 avg=2.69\n",
            "[839 | 463.08] loss=2.77 avg=2.69\n",
            "[840 | 463.53] loss=2.39 avg=2.69\n",
            "[841 | 463.99] loss=2.63 avg=2.69\n",
            "[842 | 464.45] loss=2.85 avg=2.69\n",
            "[843 | 464.90] loss=2.80 avg=2.69\n",
            "[844 | 465.36] loss=3.03 avg=2.70\n",
            "[845 | 465.81] loss=2.44 avg=2.69\n",
            "[846 | 466.27] loss=2.54 avg=2.69\n",
            "[847 | 466.73] loss=2.94 avg=2.69\n",
            "[848 | 467.19] loss=2.47 avg=2.69\n",
            "[849 | 467.64] loss=2.92 avg=2.69\n",
            "[850 | 468.10] loss=0.78 avg=2.67\n",
            "[851 | 468.56] loss=2.76 avg=2.68\n",
            "[852 | 469.02] loss=2.65 avg=2.68\n",
            "[853 | 469.48] loss=2.90 avg=2.68\n",
            "[854 | 469.93] loss=2.97 avg=2.68\n",
            "[855 | 470.39] loss=2.77 avg=2.68\n",
            "[856 | 470.85] loss=2.57 avg=2.68\n",
            "[857 | 471.31] loss=2.06 avg=2.67\n",
            "[858 | 471.76] loss=2.65 avg=2.67\n",
            "[859 | 472.22] loss=2.80 avg=2.67\n",
            "[860 | 472.68] loss=2.85 avg=2.68\n",
            "[861 | 473.14] loss=2.83 avg=2.68\n",
            "[862 | 473.59] loss=2.95 avg=2.68\n",
            "[863 | 474.05] loss=2.47 avg=2.68\n",
            "[864 | 474.50] loss=2.54 avg=2.68\n",
            "[865 | 474.96] loss=1.89 avg=2.67\n",
            "[866 | 475.42] loss=2.83 avg=2.67\n",
            "[867 | 475.88] loss=2.58 avg=2.67\n",
            "[868 | 476.33] loss=2.76 avg=2.67\n",
            "[869 | 476.79] loss=2.70 avg=2.67\n",
            "[870 | 477.25] loss=2.92 avg=2.67\n",
            "[871 | 477.71] loss=2.75 avg=2.67\n",
            "[872 | 478.17] loss=2.94 avg=2.68\n",
            "[873 | 478.63] loss=2.83 avg=2.68\n",
            "[874 | 479.09] loss=2.91 avg=2.68\n",
            "[875 | 479.54] loss=2.50 avg=2.68\n",
            "[876 | 480.00] loss=2.87 avg=2.68\n",
            "[877 | 480.46] loss=2.85 avg=2.68\n",
            "[878 | 480.92] loss=3.06 avg=2.69\n",
            "[879 | 481.37] loss=2.60 avg=2.69\n",
            "[880 | 481.84] loss=2.73 avg=2.69\n",
            "[881 | 482.29] loss=3.07 avg=2.69\n",
            "[882 | 482.76] loss=2.80 avg=2.69\n",
            "[883 | 483.22] loss=2.57 avg=2.69\n",
            "[884 | 483.68] loss=2.61 avg=2.69\n",
            "[885 | 484.13] loss=2.98 avg=2.69\n",
            "[886 | 484.59] loss=2.88 avg=2.69\n",
            "[887 | 485.05] loss=1.31 avg=2.68\n",
            "[888 | 485.51] loss=3.05 avg=2.68\n",
            "[889 | 485.97] loss=2.73 avg=2.68\n",
            "[890 | 486.43] loss=2.85 avg=2.69\n",
            "[891 | 486.88] loss=2.95 avg=2.69\n",
            "[892 | 487.34] loss=2.51 avg=2.69\n",
            "[893 | 487.80] loss=2.69 avg=2.69\n",
            "[894 | 488.25] loss=2.95 avg=2.69\n",
            "[895 | 488.71] loss=2.78 avg=2.69\n",
            "[896 | 489.17] loss=2.21 avg=2.69\n",
            "[897 | 489.63] loss=2.80 avg=2.69\n",
            "[898 | 490.09] loss=2.91 avg=2.69\n",
            "[899 | 490.54] loss=2.99 avg=2.69\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " appears to be the same as the\n",
            "conception of the existence of a being which is not a\n",
            "being of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of the conception of the conception of the\n",
            "conception of\n",
            "\n",
            "[900 | 501.21] loss=2.81 avg=2.69\n",
            "[901 | 501.68] loss=2.68 avg=2.69\n",
            "[902 | 502.14] loss=2.53 avg=2.69\n",
            "[903 | 502.59] loss=2.73 avg=2.69\n",
            "[904 | 503.05] loss=2.54 avg=2.69\n",
            "[905 | 503.51] loss=3.15 avg=2.69\n",
            "[906 | 503.97] loss=2.88 avg=2.70\n",
            "[907 | 504.43] loss=2.70 avg=2.70\n",
            "[908 | 504.89] loss=2.76 avg=2.70\n",
            "[909 | 505.34] loss=2.77 avg=2.70\n",
            "[910 | 505.80] loss=2.85 avg=2.70\n",
            "[911 | 506.26] loss=1.77 avg=2.69\n",
            "[912 | 506.72] loss=1.65 avg=2.68\n",
            "[913 | 507.17] loss=2.68 avg=2.68\n",
            "[914 | 507.63] loss=2.68 avg=2.68\n",
            "[915 | 508.09] loss=2.52 avg=2.68\n",
            "[916 | 508.54] loss=2.71 avg=2.68\n",
            "[917 | 509.00] loss=2.86 avg=2.68\n",
            "[918 | 509.46] loss=2.93 avg=2.68\n",
            "[919 | 509.92] loss=2.93 avg=2.69\n",
            "[920 | 510.37] loss=2.94 avg=2.69\n",
            "[921 | 510.83] loss=2.85 avg=2.69\n",
            "[922 | 511.29] loss=2.81 avg=2.69\n",
            "[923 | 511.75] loss=2.90 avg=2.69\n",
            "[924 | 512.20] loss=2.79 avg=2.69\n",
            "[925 | 512.66] loss=2.53 avg=2.69\n",
            "[926 | 513.12] loss=2.89 avg=2.69\n",
            "[927 | 513.58] loss=2.82 avg=2.70\n",
            "[928 | 514.03] loss=3.20 avg=2.70\n",
            "[929 | 514.49] loss=2.82 avg=2.70\n",
            "[930 | 514.95] loss=2.85 avg=2.70\n",
            "[931 | 515.40] loss=2.84 avg=2.70\n",
            "[932 | 515.86] loss=2.73 avg=2.70\n",
            "[933 | 516.32] loss=3.12 avg=2.71\n",
            "[934 | 516.78] loss=2.87 avg=2.71\n",
            "[935 | 517.23] loss=2.69 avg=2.71\n",
            "[936 | 517.69] loss=2.65 avg=2.71\n",
            "[937 | 518.14] loss=2.83 avg=2.71\n",
            "[938 | 518.60] loss=2.82 avg=2.71\n",
            "[939 | 519.07] loss=2.56 avg=2.71\n",
            "[940 | 519.52] loss=2.53 avg=2.71\n",
            "[941 | 519.98] loss=2.53 avg=2.71\n",
            "[942 | 520.44] loss=2.71 avg=2.71\n",
            "[943 | 520.90] loss=2.72 avg=2.71\n",
            "[944 | 521.35] loss=2.78 avg=2.71\n",
            "[945 | 521.82] loss=2.72 avg=2.71\n",
            "[946 | 522.28] loss=2.74 avg=2.71\n",
            "[947 | 522.73] loss=2.72 avg=2.71\n",
            "[948 | 523.19] loss=2.89 avg=2.71\n",
            "[949 | 523.65] loss=2.82 avg=2.71\n",
            "[950 | 524.11] loss=2.70 avg=2.71\n",
            "[951 | 524.57] loss=2.62 avg=2.71\n",
            "[952 | 525.03] loss=3.02 avg=2.71\n",
            "[953 | 525.49] loss=2.66 avg=2.71\n",
            "[954 | 525.94] loss=1.99 avg=2.71\n",
            "[955 | 526.40] loss=2.64 avg=2.70\n",
            "[956 | 526.86] loss=1.65 avg=2.69\n",
            "[957 | 527.32] loss=2.68 avg=2.69\n",
            "[958 | 527.77] loss=3.36 avg=2.70\n",
            "[959 | 528.23] loss=2.74 avg=2.70\n",
            "[960 | 528.69] loss=2.89 avg=2.70\n",
            "[961 | 529.15] loss=2.84 avg=2.70\n",
            "[962 | 529.60] loss=2.60 avg=2.70\n",
            "[963 | 530.06] loss=2.85 avg=2.70\n",
            "[964 | 530.52] loss=2.60 avg=2.70\n",
            "[965 | 530.98] loss=2.86 avg=2.71\n",
            "[966 | 531.44] loss=2.47 avg=2.70\n",
            "[967 | 531.89] loss=2.94 avg=2.71\n",
            "[968 | 532.35] loss=2.85 avg=2.71\n",
            "[969 | 532.81] loss=2.94 avg=2.71\n",
            "[970 | 533.27] loss=2.67 avg=2.71\n",
            "[971 | 533.72] loss=2.31 avg=2.70\n",
            "[972 | 534.18] loss=2.58 avg=2.70\n",
            "[973 | 534.64] loss=3.04 avg=2.71\n",
            "[974 | 535.10] loss=2.43 avg=2.70\n",
            "[975 | 535.56] loss=2.61 avg=2.70\n",
            "[976 | 536.02] loss=2.81 avg=2.70\n",
            "[977 | 536.48] loss=2.71 avg=2.70\n",
            "[978 | 536.93] loss=1.85 avg=2.70\n",
            "[979 | 537.39] loss=2.43 avg=2.69\n",
            "[980 | 537.84] loss=2.86 avg=2.69\n",
            "[981 | 538.30] loss=2.57 avg=2.69\n",
            "[982 | 538.76] loss=2.71 avg=2.69\n",
            "[983 | 539.22] loss=2.77 avg=2.69\n",
            "[984 | 539.68] loss=2.15 avg=2.69\n",
            "[985 | 540.13] loss=2.54 avg=2.69\n",
            "[986 | 540.59] loss=2.58 avg=2.69\n",
            "[987 | 541.05] loss=0.37 avg=2.66\n",
            "[988 | 541.51] loss=2.70 avg=2.66\n",
            "[989 | 541.97] loss=2.80 avg=2.66\n",
            "[990 | 542.43] loss=2.57 avg=2.66\n",
            "[991 | 542.89] loss=2.50 avg=2.66\n",
            "[992 | 543.35] loss=2.59 avg=2.66\n",
            "[993 | 543.81] loss=2.28 avg=2.66\n",
            "[994 | 544.27] loss=2.68 avg=2.66\n",
            "[995 | 544.73] loss=2.75 avg=2.66\n",
            "[996 | 545.19] loss=2.65 avg=2.66\n",
            "[997 | 545.64] loss=2.98 avg=2.66\n",
            "[998 | 546.11] loss=2.90 avg=2.66\n",
            "[999 | 546.56] loss=2.63 avg=2.66\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and the\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception,\n",
            "and the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of\n",
            "the object of the conception, and the conception of the object of\n",
            "the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the conception of the object of the conception, and\n",
            "the conception of the object of the conception, and the conception of the\n",
            "object of the conception, and the\n",
            "\n",
            "[1000 | 559.72] loss=2.53 avg=2.66\n",
            "[1001 | 560.19] loss=2.79 avg=2.66\n",
            "[1002 | 560.65] loss=2.84 avg=2.67\n",
            "[1003 | 561.10] loss=2.72 avg=2.67\n",
            "[1004 | 561.56] loss=3.02 avg=2.67\n",
            "[1005 | 562.01] loss=2.61 avg=2.67\n",
            "[1006 | 562.47] loss=3.11 avg=2.67\n",
            "[1007 | 562.92] loss=2.78 avg=2.67\n",
            "[1008 | 563.38] loss=2.85 avg=2.68\n",
            "[1009 | 563.84] loss=2.92 avg=2.68\n",
            "[1010 | 564.30] loss=2.73 avg=2.68\n",
            "[1011 | 564.76] loss=2.86 avg=2.68\n",
            "[1012 | 565.22] loss=2.40 avg=2.68\n",
            "[1013 | 565.67] loss=2.80 avg=2.68\n",
            "[1014 | 566.12] loss=2.87 avg=2.68\n",
            "[1015 | 566.58] loss=3.09 avg=2.69\n",
            "[1016 | 567.03] loss=2.99 avg=2.69\n",
            "[1017 | 567.49] loss=2.70 avg=2.69\n",
            "[1018 | 567.95] loss=2.39 avg=2.69\n",
            "[1019 | 568.40] loss=2.91 avg=2.69\n",
            "[1020 | 568.86] loss=2.67 avg=2.69\n",
            "[1021 | 569.32] loss=2.81 avg=2.69\n",
            "[1022 | 569.77] loss=2.50 avg=2.69\n",
            "[1023 | 570.23] loss=2.81 avg=2.69\n",
            "[1024 | 570.69] loss=2.08 avg=2.68\n",
            "[1025 | 571.14] loss=2.65 avg=2.68\n",
            "[1026 | 571.60] loss=2.82 avg=2.68\n",
            "[1027 | 572.06] loss=2.77 avg=2.68\n",
            "[1028 | 572.52] loss=3.06 avg=2.69\n",
            "[1029 | 572.97] loss=2.45 avg=2.69\n",
            "[1030 | 573.43] loss=2.81 avg=2.69\n",
            "[1031 | 573.88] loss=2.63 avg=2.69\n",
            "[1032 | 574.34] loss=2.71 avg=2.69\n",
            "[1033 | 574.80] loss=2.55 avg=2.69\n",
            "[1034 | 575.25] loss=2.94 avg=2.69\n",
            "[1035 | 575.71] loss=1.99 avg=2.68\n",
            "[1036 | 576.16] loss=2.42 avg=2.68\n",
            "[1037 | 576.63] loss=2.67 avg=2.68\n",
            "[1038 | 577.08] loss=2.81 avg=2.68\n",
            "[1039 | 577.54] loss=2.75 avg=2.68\n",
            "[1040 | 577.99] loss=2.84 avg=2.68\n",
            "[1041 | 578.45] loss=2.76 avg=2.68\n",
            "[1042 | 578.91] loss=2.64 avg=2.68\n",
            "[1043 | 579.37] loss=2.64 avg=2.68\n",
            "[1044 | 579.82] loss=2.75 avg=2.68\n",
            "[1045 | 580.28] loss=2.75 avg=2.68\n",
            "[1046 | 580.73] loss=2.55 avg=2.68\n",
            "[1047 | 581.19] loss=2.62 avg=2.68\n",
            "[1048 | 581.65] loss=2.57 avg=2.68\n",
            "[1049 | 582.11] loss=2.84 avg=2.68\n",
            "[1050 | 582.56] loss=2.85 avg=2.68\n",
            "[1051 | 583.02] loss=2.74 avg=2.68\n",
            "[1052 | 583.48] loss=2.77 avg=2.68\n",
            "[1053 | 583.93] loss=2.62 avg=2.68\n",
            "[1054 | 584.39] loss=2.89 avg=2.69\n",
            "[1055 | 584.85] loss=2.46 avg=2.68\n",
            "[1056 | 585.30] loss=2.71 avg=2.68\n",
            "[1057 | 585.76] loss=2.55 avg=2.68\n",
            "[1058 | 586.22] loss=2.63 avg=2.68\n",
            "[1059 | 586.67] loss=2.60 avg=2.68\n",
            "[1060 | 587.13] loss=3.53 avg=2.69\n",
            "[1061 | 587.59] loss=2.74 avg=2.69\n",
            "[1062 | 588.04] loss=3.06 avg=2.69\n",
            "[1063 | 588.50] loss=2.91 avg=2.70\n",
            "[1064 | 588.96] loss=2.60 avg=2.70\n",
            "[1065 | 589.42] loss=2.35 avg=2.69\n",
            "[1066 | 589.87] loss=2.68 avg=2.69\n",
            "[1067 | 590.33] loss=2.78 avg=2.69\n",
            "[1068 | 590.79] loss=2.56 avg=2.69\n",
            "[1069 | 591.24] loss=2.60 avg=2.69\n",
            "[1070 | 591.70] loss=2.38 avg=2.69\n",
            "[1071 | 592.16] loss=2.76 avg=2.69\n",
            "[1072 | 592.61] loss=2.64 avg=2.69\n",
            "[1073 | 593.07] loss=2.70 avg=2.69\n",
            "[1074 | 593.53] loss=2.91 avg=2.69\n",
            "[1075 | 593.98] loss=2.14 avg=2.68\n",
            "[1076 | 594.44] loss=2.78 avg=2.69\n",
            "[1077 | 594.90] loss=3.34 avg=2.69\n",
            "[1078 | 595.35] loss=2.85 avg=2.69\n",
            "[1079 | 595.81] loss=2.89 avg=2.70\n",
            "[1080 | 596.27] loss=2.62 avg=2.69\n",
            "[1081 | 596.73] loss=2.76 avg=2.70\n",
            "[1082 | 597.18] loss=2.52 avg=2.69\n",
            "[1083 | 597.64] loss=2.85 avg=2.70\n",
            "[1084 | 598.09] loss=2.67 avg=2.69\n",
            "[1085 | 598.55] loss=2.47 avg=2.69\n",
            "[1086 | 599.01] loss=2.68 avg=2.69\n",
            "[1087 | 599.46] loss=0.91 avg=2.67\n",
            "[1088 | 599.92] loss=2.80 avg=2.68\n",
            "[1089 | 600.38] loss=2.84 avg=2.68\n",
            "[1090 | 600.84] loss=2.87 avg=2.68\n",
            "[1091 | 601.30] loss=2.52 avg=2.68\n",
            "[1092 | 601.75] loss=2.40 avg=2.68\n",
            "[1093 | 602.21] loss=2.59 avg=2.67\n",
            "[1094 | 602.67] loss=2.58 avg=2.67\n",
            "[1095 | 603.13] loss=2.64 avg=2.67\n",
            "[1096 | 603.59] loss=2.80 avg=2.67\n",
            "[1097 | 604.04] loss=2.74 avg=2.67\n",
            "[1098 | 604.50] loss=0.32 avg=2.65\n",
            "[1099 | 604.96] loss=2.71 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "phenphenomena of the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the pure understanding, and the\n",
            "phenomena of the\n",
            "\n",
            "[1100 | 615.64] loss=2.56 avg=2.65\n",
            "[1101 | 616.11] loss=2.28 avg=2.65\n",
            "[1102 | 616.57] loss=2.36 avg=2.64\n",
            "[1103 | 617.03] loss=2.87 avg=2.65\n",
            "[1104 | 617.49] loss=2.85 avg=2.65\n",
            "[1105 | 617.95] loss=2.64 avg=2.65\n",
            "[1106 | 618.40] loss=2.52 avg=2.65\n",
            "[1107 | 618.86] loss=2.99 avg=2.65\n",
            "[1108 | 619.31] loss=2.51 avg=2.65\n",
            "[1109 | 619.77] loss=2.76 avg=2.65\n",
            "[1110 | 620.23] loss=2.76 avg=2.65\n",
            "[1111 | 620.68] loss=2.73 avg=2.65\n",
            "[1112 | 621.14] loss=2.57 avg=2.65\n",
            "[1113 | 621.60] loss=2.81 avg=2.65\n",
            "[1114 | 622.06] loss=2.64 avg=2.65\n",
            "[1115 | 622.51] loss=2.84 avg=2.65\n",
            "[1116 | 622.97] loss=3.15 avg=2.66\n",
            "[1117 | 623.43] loss=3.05 avg=2.66\n",
            "[1118 | 623.89] loss=3.01 avg=2.67\n",
            "[1119 | 624.34] loss=2.87 avg=2.67\n",
            "[1120 | 624.80] loss=2.72 avg=2.67\n",
            "[1121 | 625.27] loss=2.50 avg=2.67\n",
            "[1122 | 625.72] loss=2.64 avg=2.67\n",
            "[1123 | 626.19] loss=2.78 avg=2.67\n",
            "[1124 | 626.64] loss=2.48 avg=2.67\n",
            "[1125 | 627.10] loss=2.75 avg=2.67\n",
            "[1126 | 627.56] loss=2.62 avg=2.67\n",
            "[1127 | 628.01] loss=2.78 avg=2.67\n",
            "[1128 | 628.47] loss=2.94 avg=2.67\n",
            "[1129 | 628.93] loss=2.71 avg=2.67\n",
            "[1130 | 629.39] loss=1.78 avg=2.66\n",
            "[1131 | 629.84] loss=2.60 avg=2.66\n",
            "[1132 | 630.30] loss=2.66 avg=2.66\n",
            "[1133 | 630.76] loss=2.63 avg=2.66\n",
            "[1134 | 631.22] loss=2.40 avg=2.66\n",
            "[1135 | 631.67] loss=2.93 avg=2.66\n",
            "[1136 | 632.13] loss=2.70 avg=2.66\n",
            "[1137 | 632.59] loss=2.48 avg=2.66\n",
            "[1138 | 633.04] loss=2.65 avg=2.66\n",
            "[1139 | 633.50] loss=2.78 avg=2.66\n",
            "[1140 | 633.95] loss=2.85 avg=2.66\n",
            "[1141 | 634.41] loss=2.67 avg=2.66\n",
            "[1142 | 634.87] loss=2.86 avg=2.67\n",
            "[1143 | 635.32] loss=2.49 avg=2.66\n",
            "[1144 | 635.78] loss=2.71 avg=2.66\n",
            "[1145 | 636.24] loss=2.75 avg=2.67\n",
            "[1146 | 636.69] loss=2.59 avg=2.66\n",
            "[1147 | 637.15] loss=2.98 avg=2.67\n",
            "[1148 | 637.61] loss=2.86 avg=2.67\n",
            "[1149 | 638.07] loss=2.46 avg=2.67\n",
            "[1150 | 638.52] loss=2.50 avg=2.67\n",
            "[1151 | 638.98] loss=2.94 avg=2.67\n",
            "[1152 | 639.44] loss=2.79 avg=2.67\n",
            "[1153 | 639.90] loss=2.69 avg=2.67\n",
            "[1154 | 640.35] loss=2.80 avg=2.67\n",
            "[1155 | 640.81] loss=2.70 avg=2.67\n",
            "[1156 | 641.27] loss=2.71 avg=2.67\n",
            "[1157 | 641.73] loss=2.87 avg=2.67\n",
            "[1158 | 642.19] loss=2.42 avg=2.67\n",
            "[1159 | 642.64] loss=2.69 avg=2.67\n",
            "[1160 | 643.10] loss=2.62 avg=2.67\n",
            "[1161 | 643.56] loss=2.32 avg=2.67\n",
            "[1162 | 644.02] loss=3.05 avg=2.67\n",
            "[1163 | 644.47] loss=2.62 avg=2.67\n",
            "[1164 | 644.93] loss=2.72 avg=2.67\n",
            "[1165 | 645.39] loss=2.50 avg=2.67\n",
            "[1166 | 645.85] loss=2.41 avg=2.67\n",
            "[1167 | 646.30] loss=2.71 avg=2.67\n",
            "[1168 | 646.76] loss=2.38 avg=2.66\n",
            "[1169 | 647.22] loss=2.76 avg=2.67\n",
            "[1170 | 647.68] loss=2.72 avg=2.67\n",
            "[1171 | 648.14] loss=2.49 avg=2.66\n",
            "[1172 | 648.60] loss=2.65 avg=2.66\n",
            "[1173 | 649.06] loss=2.60 avg=2.66\n",
            "[1174 | 649.52] loss=2.73 avg=2.66\n",
            "[1175 | 649.98] loss=2.49 avg=2.66\n",
            "[1176 | 650.44] loss=2.56 avg=2.66\n",
            "[1177 | 650.89] loss=2.28 avg=2.66\n",
            "[1178 | 651.35] loss=2.65 avg=2.66\n",
            "[1179 | 651.81] loss=2.79 avg=2.66\n",
            "[1180 | 652.27] loss=2.48 avg=2.66\n",
            "[1181 | 652.73] loss=2.43 avg=2.65\n",
            "[1182 | 653.18] loss=2.55 avg=2.65\n",
            "[1183 | 653.64] loss=2.80 avg=2.66\n",
            "[1184 | 654.09] loss=2.74 avg=2.66\n",
            "[1185 | 654.56] loss=2.76 avg=2.66\n",
            "[1186 | 655.02] loss=2.96 avg=2.66\n",
            "[1187 | 655.48] loss=2.98 avg=2.66\n",
            "[1188 | 655.94] loss=2.94 avg=2.67\n",
            "[1189 | 656.39] loss=2.84 avg=2.67\n",
            "[1190 | 656.85] loss=2.92 avg=2.67\n",
            "[1191 | 657.30] loss=2.41 avg=2.67\n",
            "[1192 | 657.76] loss=2.74 avg=2.67\n",
            "[1193 | 658.22] loss=2.66 avg=2.67\n",
            "[1194 | 658.68] loss=2.33 avg=2.66\n",
            "[1195 | 659.14] loss=2.76 avg=2.67\n",
            "[1196 | 659.59] loss=2.45 avg=2.66\n",
            "[1197 | 660.05] loss=2.75 avg=2.66\n",
            "[1198 | 660.51] loss=2.69 avg=2.66\n",
            "[1199 | 660.97] loss=3.03 avg=2.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ic\n",
            "theoretical principles of the understanding, and the\n",
            "conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding, and the conception of the understanding, and the\n",
            "conception of the understanding, and the conception of the understanding\n",
            "and the conception of the understanding, and the conception of the\n",
            "understanding\n",
            "\n",
            "[1200 | 671.59] loss=2.71 avg=2.67\n",
            "[1201 | 672.06] loss=2.57 avg=2.67\n",
            "[1202 | 672.51] loss=2.63 avg=2.67\n",
            "[1203 | 672.96] loss=2.67 avg=2.67\n",
            "[1204 | 673.42] loss=2.44 avg=2.67\n",
            "[1205 | 673.88] loss=2.64 avg=2.66\n",
            "[1206 | 674.34] loss=2.80 avg=2.67\n",
            "[1207 | 674.79] loss=2.56 avg=2.67\n",
            "[1208 | 675.26] loss=2.79 avg=2.67\n",
            "[1209 | 675.71] loss=2.67 avg=2.67\n",
            "[1210 | 676.17] loss=2.65 avg=2.67\n",
            "[1211 | 676.63] loss=2.62 avg=2.67\n",
            "[1212 | 677.08] loss=2.45 avg=2.66\n",
            "[1213 | 677.54] loss=2.54 avg=2.66\n",
            "[1214 | 678.00] loss=2.36 avg=2.66\n",
            "[1215 | 678.46] loss=2.66 avg=2.66\n",
            "[1216 | 678.91] loss=3.05 avg=2.66\n",
            "[1217 | 679.38] loss=3.23 avg=2.67\n",
            "[1218 | 679.83] loss=2.77 avg=2.67\n",
            "[1219 | 680.29] loss=0.51 avg=2.65\n",
            "[1220 | 680.74] loss=2.79 avg=2.65\n",
            "[1221 | 681.20] loss=2.68 avg=2.65\n",
            "[1222 | 681.66] loss=2.06 avg=2.64\n",
            "[1223 | 682.12] loss=2.62 avg=2.64\n",
            "[1224 | 682.57] loss=2.76 avg=2.65\n",
            "[1225 | 683.03] loss=2.63 avg=2.65\n",
            "[1226 | 683.48] loss=2.39 avg=2.64\n",
            "[1227 | 683.94] loss=2.36 avg=2.64\n",
            "[1228 | 684.40] loss=2.62 avg=2.64\n",
            "[1229 | 684.86] loss=2.76 avg=2.64\n",
            "[1230 | 685.31] loss=2.67 avg=2.64\n",
            "[1231 | 685.77] loss=2.60 avg=2.64\n",
            "[1232 | 686.23] loss=2.66 avg=2.64\n",
            "[1233 | 686.69] loss=2.90 avg=2.64\n",
            "[1234 | 687.14] loss=2.50 avg=2.64\n",
            "[1235 | 687.60] loss=2.52 avg=2.64\n",
            "[1236 | 688.06] loss=2.59 avg=2.64\n",
            "[1237 | 688.51] loss=2.56 avg=2.64\n",
            "[1238 | 688.97] loss=2.74 avg=2.64\n",
            "[1239 | 689.43] loss=2.92 avg=2.64\n",
            "[1240 | 689.89] loss=2.63 avg=2.64\n",
            "[1241 | 690.34] loss=2.89 avg=2.65\n",
            "[1242 | 690.80] loss=2.76 avg=2.65\n",
            "[1243 | 691.26] loss=2.80 avg=2.65\n",
            "[1244 | 691.72] loss=3.10 avg=2.65\n",
            "[1245 | 692.17] loss=2.72 avg=2.65\n",
            "[1246 | 692.63] loss=2.86 avg=2.66\n",
            "[1247 | 693.10] loss=2.99 avg=2.66\n",
            "[1248 | 693.55] loss=2.40 avg=2.66\n",
            "[1249 | 694.01] loss=2.58 avg=2.66\n",
            "[1250 | 694.47] loss=2.64 avg=2.66\n",
            "[1251 | 694.93] loss=2.49 avg=2.65\n",
            "[1252 | 695.38] loss=2.38 avg=2.65\n",
            "[1253 | 695.84] loss=2.84 avg=2.65\n",
            "[1254 | 696.30] loss=3.27 avg=2.66\n",
            "[1255 | 696.75] loss=2.81 avg=2.66\n",
            "[1256 | 697.22] loss=2.81 avg=2.66\n",
            "[1257 | 697.67] loss=2.73 avg=2.66\n",
            "[1258 | 698.13] loss=2.49 avg=2.66\n",
            "[1259 | 698.59] loss=2.83 avg=2.66\n",
            "[1260 | 699.04] loss=2.69 avg=2.66\n",
            "[1261 | 699.50] loss=3.07 avg=2.67\n",
            "[1262 | 699.96] loss=0.23 avg=2.64\n",
            "[1263 | 700.42] loss=2.75 avg=2.64\n",
            "[1264 | 700.88] loss=2.80 avg=2.65\n",
            "[1265 | 701.33] loss=2.61 avg=2.65\n",
            "[1266 | 701.79] loss=0.33 avg=2.62\n",
            "[1267 | 702.24] loss=2.90 avg=2.62\n",
            "[1268 | 702.70] loss=2.80 avg=2.63\n",
            "[1269 | 703.15] loss=2.64 avg=2.63\n",
            "[1270 | 703.62] loss=2.59 avg=2.63\n",
            "[1271 | 704.07] loss=2.28 avg=2.62\n",
            "[1272 | 704.53] loss=2.98 avg=2.63\n",
            "[1273 | 704.99] loss=2.52 avg=2.63\n",
            "[1274 | 705.45] loss=2.89 avg=2.63\n",
            "[1275 | 705.90] loss=2.50 avg=2.63\n",
            "[1276 | 706.36] loss=2.61 avg=2.63\n",
            "[1277 | 706.82] loss=2.57 avg=2.63\n",
            "[1278 | 707.27] loss=2.61 avg=2.63\n",
            "[1279 | 707.73] loss=2.84 avg=2.63\n",
            "[1280 | 708.19] loss=2.66 avg=2.63\n",
            "[1281 | 708.65] loss=2.77 avg=2.63\n",
            "[1282 | 709.11] loss=3.12 avg=2.63\n",
            "[1283 | 709.56] loss=2.44 avg=2.63\n",
            "[1284 | 710.02] loss=2.67 avg=2.63\n",
            "[1285 | 710.48] loss=2.59 avg=2.63\n",
            "[1286 | 710.94] loss=2.91 avg=2.64\n",
            "[1287 | 711.39] loss=2.66 avg=2.64\n",
            "[1288 | 711.86] loss=2.78 avg=2.64\n",
            "[1289 | 712.32] loss=2.76 avg=2.64\n",
            "[1290 | 712.77] loss=2.65 avg=2.64\n",
            "[1291 | 713.23] loss=0.40 avg=2.62\n",
            "[1292 | 713.69] loss=2.42 avg=2.61\n",
            "[1293 | 714.15] loss=2.87 avg=2.62\n",
            "[1294 | 714.60] loss=2.78 avg=2.62\n",
            "[1295 | 715.06] loss=2.57 avg=2.62\n",
            "[1296 | 715.52] loss=3.23 avg=2.62\n",
            "[1297 | 715.98] loss=2.98 avg=2.63\n",
            "[1298 | 716.43] loss=2.75 avg=2.63\n",
            "[1299 | 716.89] loss=2.27 avg=2.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in the\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and the\n",
            "conception of the object of the conception is the only\n",
            "conception of the object of the conception.\n",
            "\n",
            "The conception of the object of the conception is the only\n",
            "conception of the object of the conception, and\n",
            "\n",
            "[1300 | 727.50] loss=2.82 avg=2.63\n",
            "[1301 | 727.97] loss=2.73 avg=2.63\n",
            "[1302 | 728.43] loss=2.69 avg=2.63\n",
            "[1303 | 728.88] loss=2.67 avg=2.63\n",
            "[1304 | 729.34] loss=2.72 avg=2.63\n",
            "[1305 | 729.80] loss=2.46 avg=2.63\n",
            "[1306 | 730.26] loss=2.66 avg=2.63\n",
            "[1307 | 730.71] loss=2.49 avg=2.63\n",
            "[1308 | 731.17] loss=1.82 avg=2.62\n",
            "[1309 | 731.62] loss=2.70 avg=2.62\n",
            "[1310 | 732.08] loss=3.12 avg=2.63\n",
            "[1311 | 732.54] loss=2.55 avg=2.62\n",
            "[1312 | 733.00] loss=2.74 avg=2.63\n",
            "[1313 | 733.45] loss=2.50 avg=2.62\n",
            "[1314 | 733.92] loss=3.24 avg=2.63\n",
            "[1315 | 734.37] loss=2.75 avg=2.63\n",
            "[1316 | 734.83] loss=2.40 avg=2.63\n",
            "[1317 | 735.29] loss=2.44 avg=2.63\n",
            "[1318 | 735.75] loss=2.63 avg=2.63\n",
            "[1319 | 736.20] loss=2.65 avg=2.63\n",
            "[1320 | 736.66] loss=3.09 avg=2.63\n",
            "[1321 | 737.12] loss=2.79 avg=2.63\n",
            "[1322 | 737.58] loss=2.89 avg=2.64\n",
            "[1323 | 738.04] loss=2.90 avg=2.64\n",
            "[1324 | 738.49] loss=2.57 avg=2.64\n",
            "[1325 | 738.95] loss=2.71 avg=2.64\n",
            "[1326 | 739.41] loss=2.81 avg=2.64\n",
            "[1327 | 739.87] loss=0.96 avg=2.62\n",
            "[1328 | 740.32] loss=2.12 avg=2.62\n",
            "[1329 | 740.78] loss=2.45 avg=2.62\n",
            "[1330 | 741.24] loss=2.67 avg=2.62\n",
            "[1331 | 741.69] loss=2.46 avg=2.62\n",
            "[1332 | 742.15] loss=2.99 avg=2.62\n",
            "[1333 | 742.61] loss=2.71 avg=2.62\n",
            "[1334 | 743.06] loss=2.45 avg=2.62\n",
            "[1335 | 743.52] loss=2.70 avg=2.62\n",
            "[1336 | 743.98] loss=2.89 avg=2.62\n",
            "[1337 | 744.44] loss=3.12 avg=2.63\n",
            "[1338 | 744.90] loss=1.05 avg=2.61\n",
            "[1339 | 745.35] loss=2.59 avg=2.61\n",
            "[1340 | 745.81] loss=2.35 avg=2.61\n",
            "[1341 | 746.27] loss=2.25 avg=2.61\n",
            "[1342 | 746.73] loss=2.76 avg=2.61\n",
            "[1343 | 747.19] loss=2.56 avg=2.61\n",
            "[1344 | 747.65] loss=2.09 avg=2.60\n",
            "[1345 | 748.10] loss=2.69 avg=2.60\n",
            "[1346 | 748.56] loss=1.88 avg=2.59\n",
            "[1347 | 749.01] loss=2.78 avg=2.60\n",
            "[1348 | 749.47] loss=2.48 avg=2.60\n",
            "[1349 | 749.93] loss=2.59 avg=2.60\n",
            "[1350 | 750.39] loss=2.79 avg=2.60\n",
            "[1351 | 750.84] loss=1.74 avg=2.59\n",
            "[1352 | 751.30] loss=2.77 avg=2.59\n",
            "[1353 | 751.75] loss=2.39 avg=2.59\n",
            "[1354 | 752.21] loss=2.21 avg=2.58\n",
            "[1355 | 752.67] loss=2.43 avg=2.58\n",
            "[1356 | 753.13] loss=2.44 avg=2.58\n",
            "[1357 | 753.58] loss=2.89 avg=2.59\n",
            "[1358 | 754.05] loss=2.50 avg=2.58\n",
            "[1359 | 754.50] loss=2.40 avg=2.58\n",
            "[1360 | 754.96] loss=2.80 avg=2.58\n",
            "[1361 | 755.42] loss=2.63 avg=2.58\n",
            "[1362 | 755.88] loss=2.67 avg=2.59\n",
            "[1363 | 756.33] loss=2.39 avg=2.58\n",
            "[1364 | 756.79] loss=2.78 avg=2.59\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title Step 9:Training the Model\n",
        "#Model saved after 1000 steps\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src/\")\n",
        "!python train.py --dataset out.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy8zCdiHRYJ7"
      },
      "source": [
        "#Step 10: Creating a training model directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-zAFd2hLQ2V"
      },
      "outputs": [],
      "source": [
        "#@title Step 10: Creating a Training Model directory\n",
        "#Creating a Training Model directory named 'tgmodel'\n",
        "import os\n",
        "run_dir = '/content/gpt-2/models/tgmodel'\n",
        "if not os.path.exists(run_dir):\n",
        "  os.makedirs(run_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-POx-g1Ql76C"
      },
      "outputs": [],
      "source": [
        "#@title Step 10A: Copying training Files\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.data-00000-of-00001 /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/checkpoint /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.index /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/src/checkpoint/run1/model-1000.meta /content/gpt-2/models/tgmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdE9nNH8m7VD"
      },
      "outputs": [],
      "source": [
        "#@title Step 10B: Copying the OpenAI GPT-2 117M Model files\n",
        "!cp /content/gpt-2/models/117M/encoder.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/hparams.json /content/gpt-2/models/tgmodel\n",
        "!cp /content/gpt-2/models/117M/vocab.bpe /content/gpt-2/models/tgmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G8NOUXjMq4u"
      },
      "outputs": [],
      "source": [
        "#@title Step 10C: Renaming the model directories\n",
        "import os\n",
        "!mv /content/gpt-2/models/117M  /content/gpt-2/models/117M_OpenAI\n",
        "!mv /content/gpt-2/models/tgmodel  /content/gpt-2/models/117M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3uexz_e4d18"
      },
      "source": [
        "#Step 11: Generating unconditional samples\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python generate_unconditional_samples.py --model_name '117M'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys6nZM98dRX_",
        "outputId": "e447b3f3-afbf-4fd9-c544-c26020a11e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================== SAMPLE 4 ========================================\n",
            "Malcolm Turnbull has slammed a plan to bar most union members from working on technology that effectively targets her interest in data protection under one of her stinging dismissal letters to the federal unions.\n",
            "\n",
            "Labor failures and signs of change feel 'disgusting' after two Liberal MPs critical of Abbott's digital privacy policy send denials campaign\n",
            "\n",
            "Zaharie Leonhardt (left) and Michael Hoy (right) reportedly received comments about the reform from the HSL. Photo: WAYNE HENRY Melbourne minister, Mumta Serena, says requests to remove Palmer voted against by 22 to 16 votes. The salary cap for politicians are set to rise to $13,000 by 2017 after government-appointed director Sally Critchuk's move came under intense pressure in her summer and fall term. Labor had offered to leave Victorian election campaign worker Grant Smith for a seat on the triple ABC review committee after five of his colleagues told him he was too important to lose their projects, without offering to join the review. He was given a day off, instead. Wikileaks founder Julian Assange posted Private industry executives Add campaign for reform to his blog in protest of comments from Tasmanian teacher Ciaran Armstrong that \"telecommunications and the creative professions should be given First Class Officer status\" it claims. Is it surprising as Home Minister Julia Gillard has welcomed swift progress in a drive to proactively save the workforce? Liberal Leader Bill Shorten, a key member of the Howard government's celebrated White House team earlier this month, called the Telecommunications and Communications Agreement (TTIP) implementation scam to General Education Advisory Group figures. \"What the evidence shows to us is shockingly few providers who have actually implemented TIP are green button assholes like Google or Telstra,\" Shorten said. \"Clearly there is a massive disconnect between industry and policy, but we have to decide which one we begin this process on so that we have a clear identity for the country.\" Earlier on Monday a letter sent by RTÉ boss McNamara pleaded with the history minister \"to cover his tracks\". Among other demands is that RTÉ \"do more to prevent data retention of employees that increases our the resilience\" and \"provide more Bureaucratic review to ensure employees remain safe and responsible.\" In a first in her life Opposition Communications Minister Malcolm Turnbull's office refuses to confirm RTÉ's level of compliance to protect public interests. Communications spokeswoman Priscilla Colvin says the spokeswoman is who has been concerned. \"Former CSO Terence Saasart, others will have the process straight. We need to think EEC practices are practised.\" An RTÉ spokeswoman says neither Ms Colvin nor Ms Turnbull is willing to comment on precisely how RTÉ has complied with LMQ's requests. \"The transparency issue is addressing every aspect of an individual in every part of the company - whether it is a push to remove a PR person or a management change. \"It has taken the integrity of investigations into over 200 employees to bring the creative professions, and city and county governments in general, to light. Happy to amidsay.\" Fake stories WSJ licence Thordes PR could be in a position to set off a strike strike in response to labour problems\n",
            "\n",
            "State NAT volunteers engage places2i in health reforms\n",
            "\n",
            "FL leadership contest goes enelnet on ABC PSA NSW leadership race\n",
            "\n",
            "Like this? Consider supporting these activism resources in your social media: Facebook: bribery Miracle directories distortion encoding surrender Bond joy achieveime Don't drag @nabc into negotiations with ABC whereld.org?ch is needed Personnel_Danes introduce amendments superdusting cap hug capitalism breathholes\n",
            "\n",
            "Writes Katy Barton now will wrestle Picdirol sit down: @maejbenee\n",
            "\n",
            "Supreme Court protests Court's personal equity testing agenda running Lethal pioneered conspicuouscarry.obj cheat robbery passive Cowaniel describs backtracking policy policy on mistrial, blames national amboy for news NH Walmart backlash price change harried ME CLINTON say button assholes have sacrificed millions Of occupational freedom comes at any price Handels able to \"complete\" political machine couldisman Andrea Combs B-list fatters Brutality forced cars to racistlevel deluded Obama's unyielding approach towards disability exposed Voter fraud turns down Koch right to stay over Left says she won't witness close Liberal coup ship ballots I like the focus FocusEconomics authorities get hamsometimes tax fresh snake bit Purche meting Mr Trump Dig David Phillips + Trump act signals calls to act for stream toilets return Bill Biden play Clarkson sign chief one reform has no effect tournament modern art kudzu Wiener strike demonst windham gives DEF false Solomon Hadley, deputised judge, is lesbian marriage Bill, men promised safety law Thunderers fly half his body around court warning Mayor Only when branching from Rozel to Silijtor Catalina creameline rICH learn sick to listen Stress depletion shuts field day last mambo clip dates for Exalambouring events George Llooper that women should throw down drum music brain game wins game sick snorter heartcard sgordy dream\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python generate_unconditional_samples.py --model_name '117M'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byhA0qukRjJp"
      },
      "source": [
        "#Step 12: Interactive context and completion examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HI7DuBK4iSU",
        "outputId": "ba7df98e-a463-47da-f6b9-c0cb1a4a150c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-06-17 10:13:24.531482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-06-17 10:13:24.558429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.559022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-06-17 10:13:24.559380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-06-17 10:13:24.561035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-06-17 10:13:24.562597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-06-17 10:13:24.562925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-06-17 10:13:24.564438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-06-17 10:13:24.565137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-06-17 10:13:24.567984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-06-17 10:13:24.568105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.568709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.569216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-06-17 10:13:24.569564: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n",
            "2021-06-17 10:13:24.573608: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000194999 Hz\n",
            "2021-06-17 10:13:24.573793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559017c26d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-17 10:13:24.573820: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-06-17 10:13:24.672175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.672893: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559017c26bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-06-17 10:13:24.672925: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-06-17 10:13:24.673089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.673652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-06-17 10:13:24.673721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-06-17 10:13:24.673745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-06-17 10:13:24.673765: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-06-17 10:13:24.673784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-06-17 10:13:24.673807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-06-17 10:13:24.673826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-06-17 10:13:24.673845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-06-17 10:13:24.673917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.674470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.674969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-06-17 10:13:24.675034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-06-17 10:13:24.676149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-06-17 10:13:24.676176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-06-17 10:13:24.676190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-06-17 10:13:24.676305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.676886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-17 10:13:24.677388: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-06-17 10:13:24.677428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Model prompt >>> Human reason, in one sphere of its cognition, is called upon to consider questions, which it cannot decline, as they are presented by its own nature, but which it cannot answer, as they transcend every faculty of the mind.\n",
            "2021-06-17 10:13:41.056992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "======================================== SAMPLE 1 ========================================\n",
            " The first question, to which it is obliged to respond, is, how can it be true that a mind which is ignorant of the truth of any proposition can, or ought, to, accept any proposition which it has not given it? And this question has been so well answered that it is impossible for any man to believe in the existence of any mind which has never received any information. How can a man who is ignorant of the truth of any proposition accept a proposition which he does not understand? And this is the very question which has been so frequently answered, that it is the most difficult to believe in the existence of any mind which does not receive it.\n",
            "\n",
            "The philosophers have, in their great study of the subject, to deal with the question of the mind at all. The philosophers have not done so well, because they have not dealt with the subject with an impartial spirit. They have not engaged in a great deal of investigation, because they have not yet come to an end. But they have in their philosophy an important object, which is, that every man may be persuaded to consider the existence of any mind which he does not understand. They have, therefore, so much as they have to say on this subject, and so little to say on the other. And this very matter is the subject as a whole, and not only on the ground, that, no matter how much they speak of the subject, they have not treated it with a moral, and have not even tried to determine the subject by it.\"\n",
            "\n",
            "\"It is not the mind of any man, as far as we can judge, which is the subject of any philosophical inquiry. It is the mind of the minds of men, in their opinion, which they consider the most to be their most important subject. And if they can see through this, they will see it, and they will understand it, and they will understand it.\"\n",
            "\n",
            "\"You see, then, that the mind of any man is not the object of any philosophical inquiry. You are, indeed, able to see through all the world, and even all the facts which come up before you. And if you can see through the world of things, you will know how very much they are; and, if anything happens to the world, you will understand what it is.\"\n",
            "\n",
            "\"Then you do not know, then, that the mind of any man is not the object of any philosophical inquiry. That is the mind of a man, that is, the mind of a man\n",
            "================================================================================\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 5480, in get_controller\n",
            "    yield g\n",
            "  File \"interactive_conditional_samples.py\", line 73, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"interactive_conditional_samples.py\", line 91, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 471, in _Fire\n",
            "    target=component.__name__)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fire/core.py\", line 681, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"interactive_conditional_samples.py\", line 88, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\", line 1633, in __exit__\n",
            "    close_thread.start()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 857, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 552, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 296, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title Step 12: Interactive Context and Completion Examples\n",
        "import os # import after runtime is restarted\n",
        "os.chdir(\"/content/gpt-2/src\")\n",
        "!python interactive_conditional_samples.py --temperature 0.8 --top_k 40 --model_name '117M'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
